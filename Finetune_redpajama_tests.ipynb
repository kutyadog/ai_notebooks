{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kutyadog/ai_notebooks/blob/main/Finetune_redpajama_tests.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zF_Lt1sBuKty"
      },
      "source": [
        "# Finetuning RedPajama (\"OpenLlama\") - CJ\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TBlbTV8FuKt1"
      },
      "source": [
        "July 2023 -\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f7oFryQruKt2"
      },
      "source": [
        "## Setup\n",
        "Configure the base model and a few other variables that we'll use later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LkWk48RxuKt2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa3a07d7-202c-48b9-b741-29ab989ee30d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('togethercomputer/RedPajama-INCITE-Chat-3B-v1',\n",
              " 'johnrobinsn/alpaca-cleaned',\n",
              " 'redpj7B-lora-int8-alpaca',\n",
              " 'redpj7B-lora-int8-alpaca')"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "model = '7B' #'3B' #'7B' # Pick your poison\n",
        "\n",
        "if model == '7B':\n",
        "    # model_name = (\"togethercomputer/RedPajama-INCITE-Base-7B-v0.1\",\"togethercomputer/RedPajama-INCITE-Base-7B-v0.1\")\n",
        "    model_name = (\"togethercomputer/RedPajama-INCITE-Chat-3B-v1\",\"togethercomputer/RedPajama-INCITE-Chat-3B-v1\")\n",
        "    run_name = 'redpj7B-lora-int8-alpaca'\n",
        "    dataset = 'johnrobinsn/alpaca-cleaned'\n",
        "    peft_name = 'redpj7B-lora-int8-alpaca'\n",
        "    output_dir = 'redpj7B-lora-int8-alpaca-results'\n",
        "else: #3B\n",
        "    model_name = (\"togethercomputer/RedPajama-INCITE-Base-3B-v1\",\"togethercomputer/RedPajama-INCITE-Base-3B-v1\")\n",
        "    run_name = 'redpj3B-lora-int8-alpaca'\n",
        "    dataset = 'johnrobinsn/alpaca-cleaned'\n",
        "    peft_name = 'redpj3B-lora-int8-alpaca'\n",
        "    output_dir = 'redpj3B-lora-int8-alpaca-results'\n",
        "\n",
        "model_name[1],dataset,peft_name,run_name"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8iUI0flquKt3"
      },
      "source": [
        "Install the required dependencies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CN0hOIeouKt4"
      },
      "outputs": [],
      "source": [
        "def install_dependencies():\n",
        "    !pip install -Uqq  git+https://github.com/huggingface/peft.git\n",
        "    !pip install -Uqq transformers datasets accelerate bitsandbytes\n",
        "    !pip install -Uqq wandb\n",
        "\n",
        "# uncomment the following line to install the required dependencies\n",
        "install_dependencies()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zXGmP0oSuKt4"
      },
      "source": [
        "__Note: If you just want to do inference you can jump all the way down to the [\"Evaluate\"](#evaluate) cell and start running from there to download my adapter weights from HF hub and try some prompts through the finetuned model.__\n",
        "\n",
        "But if you want to train keep going..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oIqqO723uKt4"
      },
      "source": [
        "## Setting Up Tracking and Monitoring using Weights and Biases\n",
        "\n",
        "This notebook has support for logging the training run to [weights and biases (wandb)](https://wandb.ai/site).  This makes it very easy to track, monitor and annotate your training sessions from anywhere.  \n",
        "\n",
        "Run the next cell and follow the directions to authenticate with wandb."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "LALmsiOtuKt5"
      },
      "outputs": [],
      "source": [
        "report_to = \"wandb\" # \"none\"\n",
        "\n",
        "if report_to != \"none\":\n",
        "    import wandb\n",
        "    wandb.login()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ptGuSKKEuKt5"
      },
      "source": [
        "After authenticating, we have to initialize wandb.  We add a few key-value pairs about the run to the information that will be logged to the wandb dashboard.  \n",
        "\n",
        "_Note: You can add more key/values if you'd like._"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tUMkmZHwuKt5"
      },
      "outputs": [],
      "source": [
        "wandb.init(project=run_name,config={\n",
        "    \"model\": model_name[1],\n",
        "    \"dataset\":dataset\n",
        "})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Md3RzMK2uKt5"
      },
      "source": [
        "After you get training started below.  You can revisit the wandb links shown above to monitor the status of your training run from anywhere with Internet connectivity.  \n",
        "\n",
        "_Note: I like to send the link (View run) to my phone so that I can monitor on the go..._"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dZIcXuTNuKt5"
      },
      "source": [
        "## Tokenizer\n",
        "The tokenizer converts words into a list/tensor of numbers so that the model can process them.  Each language model has been trained using a specific tokenizer.  If your base model is already supported by HuggingFace then the transformer library makes it very easy to load the correct tokenizer for your given model.  Just use the AutoTokenizer class to create an instance of the correct tokenizer by just specifying the model name."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_wa0cF4_uKt5"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "print(\"Loading tokenizer for model: \", model_name[1])\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name[1],add_eos_token=True)\n",
        "tokenizer.pad_token_id = 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tHxmcFX6uKt5"
      },
      "source": [
        "One problem that I've found with many of the finetuning scripts and notebooks found online is that the \"end-of-stream\" handling is not done correctly, so in many cases the finetuned models don't know when to stop emitting tokens and tend to \"blabber\" on.  Since we are finetuning on an instruction following task, we would like the model to respond to the instruction prompt succintly and then stop.  There are a number of ways to approach this, but the way I approach it here is to explicitly add a new token to represent end-of-stream, &lt;eos&gt; and use that eos token during training to teach the model when it should stop. Then during inference, we can use that token to recognize when the model is done responding."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xx_yXO8RuKt6",
        "outputId": "e547beab-3034-42fe-b225-778e1edb8fdf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "eos_token_id: 50277\n"
          ]
        }
      ],
      "source": [
        "tokenizer.add_special_tokens({'eos_token':'<eos>'})\n",
        "print('eos_token_id:',tokenizer.eos_token_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QwcCJQz-uKt6"
      },
      "outputs": [],
      "source": [
        "CUTOFF_LEN = 256  # 256 accounts for about 96% of the data in the alpaca dataset\n",
        "\n",
        "def tokenize(prompt, tokenizer,add_eos_token=True):\n",
        "    result = tokenizer(\n",
        "        prompt+\"<eos>\",  # add the end-of-stream token\n",
        "        truncation=True,\n",
        "        max_length=CUTOFF_LEN,\n",
        "        padding=\"max_length\",\n",
        "    )\n",
        "    return {\n",
        "        \"input_ids\": result[\"input_ids\"],\n",
        "        \"attention_mask\": result[\"attention_mask\"],\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T-oygQBOuKt6"
      },
      "source": [
        "Let's give it a quick try and note the <eos> token id at the end of the sequence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mph7fL31uKt6",
        "outputId": "6888dcea-1633-4aab-a922-8c1762cc768b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': [5801, 627, 50277], 'attention_mask': [1, 1, 1]}"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "tokenizer('hi there<eos>')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7BJLaVAiuKt7"
      },
      "source": [
        "## Dataset\n",
        "\n",
        "When finetuning your model the dataset that you choose has to be aligned with your downstream task. We're using a popular Instruction Following dataset, called Alpaca. For convenience, I have a copy of the alpaca dataset that has been cleaned and published on the HuggingFace hub. We can just download it and access it from cache using the load_dataset API shown below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mewd7KXWuKt7"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Load dataset from the hub\n",
        "data = load_dataset(dataset)\n",
        "data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h0rlJ5hfnz1b",
        "outputId": "e6e60fdb-ed88-4898-c1d9-423bd9632181"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['input', 'output', 'instruction'],\n",
              "        num_rows: 51942\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5RZQgw7AuKt7"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-6fvWYQ7uKt7"
      },
      "source": [
        "We can see that the dataset consists of 51,942 rows with the following features ['instruction','input','output'].  Let's take a look at one."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hUADjOpVuKt7",
        "outputId": "71bba6a7-35c0-46bd-f2f5-3d91a19130e5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input': 'The importance of using renewable energy',\n",
              " 'output': 'The use of renewable energy is growing rapidly in relevance and importance as the world looks towards solutions to combat climate change. Renewable energy sources, such as solar, wind, and hydropower, are sustainable and have a much lower environmental impact than traditional energy sources like coal and oil. Switching to renewable energy can reduce carbon emissions, improve air quality, lessen the dependence on non-renewable resources, and provide a stable and secure energy future. Renewable energy is essential for achieving a sustainable future and should be an important part of any country’s energy portfolio.',\n",
              " 'instruction': 'Write a short paragraph about the given topic.'}"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "data['train'][45]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZQg27Ny-uKt7"
      },
      "source": [
        "We can see an item that includes an 'instruction' to direct our model.  An optional 'input' which provides context to the instruction.  And then an expected output for the model.\n",
        "\n",
        "Our goal in finetuning our model is to use this dataset to train our model to \"behave\" in a similar way.  Given an instruction respond with an appropriate response generalizing to the knowledge already encoded in the base model.\n",
        "\n",
        "But we can't directly use this JSON object to train our model.  Our model can only process an ordered sequence of tokens that represent words.  So we use a \"prompt template\" to convert each of these JSON objects in our dataset into a sequence of words.  The prompt template follows a consistent pattern."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wRiMQZLjuKt7"
      },
      "outputs": [],
      "source": [
        "def generate_prompt(data_point):\n",
        "    # sorry about the formatting disaster gotta move fast\n",
        "    if data_point[\"input\"]:\n",
        "        return f\"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "{data_point[\"instruction\"]}\n",
        "\n",
        "### Input:\n",
        "{data_point[\"input\"]}\n",
        "\n",
        "### Response:\n",
        "{data_point[\"output\"]}\"\"\"\n",
        "    else:\n",
        "        return f\"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "{data_point[\"instruction\"]}\n",
        "\n",
        "### Response:\n",
        "{data_point[\"output\"]}\"\"\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pGVSK0L6uKt7"
      },
      "source": [
        "Let's see what what our example looks like when \"templatized\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DpbD89yEuKt8",
        "outputId": "d479ab35-4f50-4b94-cb81-581e09610d3f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Identify the odd one out.\n",
            "\n",
            "### Input:\n",
            "Twitter, Instagram, Telegram\n",
            "\n",
            "### Response:\n",
            "Telegram\n"
          ]
        }
      ],
      "source": [
        "print(generate_prompt(data['train'][5]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O0hbS-P9uKt8"
      },
      "source": [
        "The exact wording of the template is somewhat arbitrary.  It's more of a consistent pattern that after training will drive the model into responding similarly when exposed to a similar prompt.  You should be able to pick out the \"instruction\", \"input\", and \"output\" from the example.  \n",
        "\n",
        "It is important that the output from the dataset is at the end of templatized prompt, since at inference time we will only provide the prompt up to **but not including the output**.  We'll expect our model to respond to our instruction on its own.\n",
        "\n",
        "We now split out a validation dataset from our training dataset. so that we can track how well the finetuning process is learning to generalize to unseen prompts and so that we make sure we're only checkpointing our model when the validation loss is improving."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uLVx7r3muKt8"
      },
      "outputs": [],
      "source": [
        "VAL_SET_SIZE = 2000  # we set aside 2000 items from our dataset for validation during training\n",
        "\n",
        "train_val = data[\"train\"].train_test_split(\n",
        "    test_size=VAL_SET_SIZE, shuffle=True, seed=42\n",
        ")\n",
        "train_data = train_val[\"train\"]\n",
        "val_data = train_val[\"test\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VXrk0oKpuKt8"
      },
      "source": [
        "We prepare the training dataset and the validation dataset by running the data through the prompt templating process and then by tokenizing the prompts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ce3hMTkwuKt8",
        "outputId": "2f1963d2-6f8f-48a7-97cc-e17b85e4263e",
        "colab": {
          "referenced_widgets": [
            "d0508368ab954234be239154b2c49298",
            "1301d396d7fd4fb0bb5d5f159695025b",
            "6f38370a8b1848e6a0e00f5fb4afe5e0",
            "d225b2606c6c4e45ab72fe87d194c4f2",
            "e71b2af50d4f4b3bbc8ebf07c39a1651",
            "d60461374273450fb04a95046ec936b9",
            "3363b42c37e541a2b3ff03a0563189a6",
            "bbc056a3090c428080bba99cb843c699",
            "0fcdabc3c6634cb3bbd2e090147d341e",
            "037a209ff570467f9ce83d224afd873f",
            "6b52943dc06442f5be21797cff61900b",
            "f98ce1ec9b07466a96fecfe3943baaa1",
            "f96b64fc475c41efb1e86e4ecd7f4ccb",
            "acbf51452aa54d9c90f5cb08f108ca39",
            "ebdd0bac53d7472a95f48c6137fff610",
            "9ebe6403e0b14f0997f6b8fcbbb49ba8",
            "1b6928fee2674c98a5123b51f886eae9",
            "2c749fc7a1f0459aaadfadae75e8887f",
            "42027765c2c34069abcf79f5f6df84c7",
            "9a2c3c39ae6840cb86144d81d2cbb7bf",
            "f0eb2a06cea84b9ab952449e6afdbd70",
            "cab4dced8ab040cc9dc008d1fc5ec2b2"
          ],
          "base_uri": "https://localhost:8080/",
          "height": 17
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/49942 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d0508368ab954234be239154b2c49298"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/2000 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f98ce1ec9b07466a96fecfe3943baaa1"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "train_data = train_data.shuffle().map(lambda x: tokenize(generate_prompt(x), tokenizer))\n",
        "val_data = val_data.shuffle().map(lambda x: tokenize(generate_prompt(x), tokenizer))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gfV_x7PGuKt8"
      },
      "source": [
        "## Load and Configure the Model for Training\n",
        "\n",
        "Load the specified RedPajama base model from the HuggingFace hub.\n",
        "\n",
        "_Note: Llama, Redpajama and other decoder-only models are supported by the AutoModelForCausalLM class.  But for encoder-decoder models such as the [**google/t5**](https://huggingface.co/google/flan-t5-xxl) models you'll need to use the AutoModelForSeq2SeqLM class and the training details are a little bit different.  Here is a [similar notebook](https://github.com/johnrobinsn/flan_ul2/blob/main/train-peft-flan-ul2-int8-alpaca.ipynb) for finetuning t5* models._"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eM6e5X-UuKt8"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM\n",
        "\n",
        "print(\"Loading model for model: \", model_name[0])\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name[0],\n",
        "    load_in_8bit=True,\n",
        "    device_map=\"auto\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "80H7yNU8uKt8"
      },
      "source": [
        "Now, we can prepare our model for the LoRA int-8 training using the HF peft library."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F-dqC9-iuKt8",
        "outputId": "e39a6408-e910-4928-cdc0-2e5710fca2ee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "trainable params: 2621440 || all params: 2778485760 || trainable%: 0.09434779323828531\n"
          ]
        }
      ],
      "source": [
        "from peft import LoraConfig, get_peft_model, prepare_model_for_int8_training, TaskType\n",
        "\n",
        "# Define LoRA Config\n",
        "lora_config = LoraConfig(\n",
        " r= 8,\n",
        " lora_alpha=16,\n",
        " target_modules=[\"query_key_value\"],\n",
        " lora_dropout=0.05,\n",
        " bias=\"none\",\n",
        " task_type=TaskType.CAUSAL_LM\n",
        ")\n",
        "\n",
        "# prepare int-8 model for training\n",
        "model = prepare_model_for_int8_training(model)\n",
        "\n",
        "# add LoRA adaptor\n",
        "model = get_peft_model(model, lora_config)\n",
        "model.print_trainable_parameters()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vXdLPG8cuKt8"
      },
      "source": [
        "_Note: After installing the Lora Adapters into the model notice the significant reduction in the number of trainable paramters._\n",
        "\n",
        "We'll leverage the training loop from the transformers library since it does a pretty good job with handling the details."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GIrigEqBuKt8"
      },
      "outputs": [],
      "source": [
        "import transformers\n",
        "eval_steps = 200\n",
        "save_steps = 200\n",
        "logging_steps = 20\n",
        "\n",
        "trainer = transformers.Trainer(\n",
        "    model=model,\n",
        "    train_dataset=train_data,\n",
        "    eval_dataset=val_data,\n",
        "    args=transformers.TrainingArguments(\n",
        "        num_train_epochs=3,\n",
        "        learning_rate=3e-4,\n",
        "        logging_steps=logging_steps,\n",
        "        evaluation_strategy=\"steps\",\n",
        "        save_strategy=\"steps\",\n",
        "        eval_steps=eval_steps,\n",
        "        save_steps=save_steps,\n",
        "        output_dir=output_dir,\n",
        "        report_to=report_to if report_to else \"none\",\n",
        "        save_total_limit=3,\n",
        "        load_best_model_at_end=True,\n",
        "        push_to_hub=False,\n",
        "        auto_find_batch_size=True\n",
        "    ),\n",
        "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
        ")\n",
        "\n",
        "model.config.use_cache = False  # silence the warnings. Please re-enable for inference!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-9Qq7k-6uKt8"
      },
      "source": [
        "## Train\n",
        "Run the training loop."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "id": "ODLwr3cbuKuF"
      },
      "outputs": [],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e21WV4CTuKuF"
      },
      "source": [
        "## Save the Trained Adpater Model to Disk\n",
        "\n",
        "Now that we've trained the model we'll want to save our weights.  First I demonstrate how to save them to disk."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IX1AA3Q4uKuF"
      },
      "outputs": [],
      "source": [
        "# Save our LoRA model & tokenizer results\n",
        "trainer.model.save_pretrained(peft_name)\n",
        "tokenizer.save_pretrained(peft_name)\n",
        "\n",
        "# if you want to save the base model to disk call\n",
        "# trainer.model.base_model.save_pretrained(peft_model_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i9u8uj6guKuF"
      },
      "source": [
        "## Push the Trained Adapter Model to the HuggingFace Hub\n",
        "\n",
        "Even better than saving your trained weights to disk you can push them up the HuggingFace Hub.  This makes it super easy to share your trained adapter with others or to setup your model for inference on other devices."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "6BvvbBWsuKuF"
      },
      "outputs": [],
      "source": [
        "!pip install -Uqq huggingface_hub\n",
        "import huggingface_hub\n",
        "huggingface_hub.login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sHYP_rk8uKuF"
      },
      "outputs": [],
      "source": [
        "# If you don't already have the git extensions for large file storage you might have to install it now.\n",
        "# Here is how you can do this for Linux from the shell.  For other OSs please refer to the git-lfs documentation.\n",
        "# sudo apt install git-lfs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ES3enxTuKuG"
      },
      "outputs": [],
      "source": [
        "repo_id = f'{huggingface_hub.whoami()[\"name\"]}/{peft_name}'\n",
        "trainer.model.push_to_hub(repo_id)\n",
        "tokenizer.push_to_hub(repo_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xh2J8DHuuKuG"
      },
      "source": [
        "You chould be able to check out HuggingFace and see your LoRA Adapter Model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ln4kJp9zuKuG"
      },
      "source": [
        "## Free Up Memory\n",
        "\n",
        "Since we likely used a lot of memory during training and we'll need that memory back to try the model out we take a few steps to free up VRAM here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nbQqhcA8uKuG"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import gc\n",
        "config = None\n",
        "model = None\n",
        "tokenizer=None\n",
        "trainer=None\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "K7-qE1g8uKuG"
      },
      "source": [
        "## Evaluate\n",
        "Here we'll try out the model for inference."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ucpGNd5MuKuG"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from peft import PeftModel, PeftConfig\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xDafcX3UuKuG",
        "outputId": "91876985-7bfc-46bb-f170-9054d6d5b090"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Overriding torch_dtype=None with `torch_dtype=torch.float16` due to requirements of `bitsandbytes` to enable model loading in mixed int8. Either pass torch_dtype=torch.float16 or don't pass this argument at all to remove this warning.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "GPTNeoXForCausalLM(\n",
              "  (gpt_neox): GPTNeoXModel(\n",
              "    (embed_in): Embedding(50432, 2560)\n",
              "    (layers): ModuleList(\n",
              "      (0-31): 32 x GPTNeoXLayer(\n",
              "        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
              "        (post_attention_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
              "        (attention): GPTNeoXAttention(\n",
              "          (rotary_emb): RotaryEmbedding()\n",
              "          (query_key_value): Linear8bitLt(in_features=2560, out_features=7680, bias=True)\n",
              "          (dense): Linear8bitLt(in_features=2560, out_features=2560, bias=True)\n",
              "        )\n",
              "        (mlp): GPTNeoXMLP(\n",
              "          (dense_h_to_4h): Linear8bitLt(in_features=2560, out_features=10240, bias=True)\n",
              "          (dense_4h_to_h): Linear8bitLt(in_features=10240, out_features=2560, bias=True)\n",
              "          (act): GELUActivation()\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (final_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (embed_out): Linear(in_features=2560, out_features=50432, bias=False)\n",
              ")"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# load base LLM model and tokenizer\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name[0],\n",
        "    load_in_8bit=True,\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name[1])\n",
        "tokenizer.pad_token_id = 0\n",
        "tokenizer.add_special_tokens({'eos_token':'<eos>'})\n",
        "\n",
        "model.eval()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GsZqzp-_uKuG"
      },
      "source": [
        "Here is the prompt template we'll use for inference.\n",
        "\n",
        "_Note: It's important that it's identical to one we used for training above, but it omits the \"output/response\" as our model will generate that for us._"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2GlSmEV5uKuG"
      },
      "outputs": [],
      "source": [
        "def generate_prompt(data_point):\n",
        "    # sorry about the formatting disaster gotta move fast\n",
        "    if data_point[\"input\"]:\n",
        "        return f\"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "{data_point[\"instruction\"]}\n",
        "\n",
        "### Input:\n",
        "{data_point[\"input\"]}\n",
        "\n",
        "### Response:\"\"\"\n",
        "    else:\n",
        "        return f\"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "{data_point[\"instruction\"]}\n",
        "\n",
        "### Response:\"\"\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jm9ntFpKuKuG"
      },
      "source": [
        "Here is a small utility function that lets us easily prompt our model with an instruction and an optional input.  It handles templating the prompt, tokenizing the templatized prompt, decoding the result and then finally stripping off the prompt from the response and just leaving us with the model response."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ybn3-QafuKuG"
      },
      "outputs": [],
      "source": [
        "def generate(instruction,input=None,maxTokens=256):\n",
        "    prompt = generate_prompt({'instruction':instruction,'input':input})\n",
        "    input_ids = tokenizer(prompt, return_tensors=\"pt\", truncation=True).input_ids.cuda()\n",
        "    outputs = model.generate(input_ids=input_ids, max_new_tokens=maxTokens,\n",
        "                             do_sample=True, top_p=0.9,pad_token_id=tokenizer.eos_token_id,\n",
        "                             forced_eos_token_id=tokenizer.eos_token_id)\n",
        "    outputs = outputs[0].tolist()\n",
        "    # Stop decoding when hitting the EOS token\n",
        "    if tokenizer.eos_token_id in outputs:\n",
        "        eos_index = outputs.index(tokenizer.eos_token_id)\n",
        "        decoded = tokenizer.decode(outputs[:eos_index])\n",
        "        # Don't show the prompt template\n",
        "        sentinel = \"### Response:\"\n",
        "        sentinelLoc = decoded.find(sentinel)\n",
        "        if sentinelLoc >= 0:\n",
        "            print(decoded[sentinelLoc+len(sentinel):])\n",
        "        else:\n",
        "            print('Warning: Expected prompt template to be emitted.  Ignoring output.')\n",
        "    else:\n",
        "        print('Warning: no <eos> detected ignoring output')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KW3oxblbuKuG"
      },
      "source": [
        "### Generating using the Base Model\n",
        "\n",
        "This demonstrates the behavior of the RedPajama model with no finetuning applied.\n",
        "\n",
        "**BEFORE FINETUNING**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z7KMvhJKuKuG",
        "outputId": "2ee3d396-4347-4aa9-81a9-28b5017009c8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Write a short story in third person narration about a protagonist who has to make an important career decision. The protagonist’s character is presented from the point of view of the protagonist. The first paragraph should describe a decision the protagonist made. In the second paragraph, the reader should learn more about the protagonist and why she made this decision. In the last paragraph, the reader should learn more about what the protagonist decided.\n",
            "\n",
            "### Examples:\n",
            "Write about a character from a novel who makes an important decision. \n",
            "Write about a character from a film that makes an important decision.\n",
            "Write about a character from a television show that makes an important decision.\n",
            "\n",
            "# **Writing Prompt 13: Write a Short Story**\n",
            "\n",
            "### Instructions:\n",
            "In the following prompt, write a short story in third person narration. The story can take place in the past or in the present. Write a story that contains:\n",
            "\n",
            "- An unreliable narrator\n",
            "- A dramatic situation\n",
            "- A situation that takes place during a specific time\n",
            "\n",
            "### Response:\n",
            "Write a short story in third person narration about a character who finds an important item. The character finds the item during a specific time. The story contains the following characteristics:\n",
            "\n",
            "- An unreliable narrator\n",
            "- A dramatic situation\n",
            "- A situation that takes place during a specific time\n",
            "\n",
            "### Examples:\n",
            "Write a short story about a character who finds an important item during a specific time.\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(42)\n",
        "generate('Write a short story in third person narration about a protagonist who has to make an important career decision.',maxTokens=300)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WChXY_KbuKuG"
      },
      "source": [
        "### Load the LoRA Adapter\n",
        "\n",
        "As you can see the generated text doesn't seem very responsive to the prompt.  Now let's load the trained LoRA adapter and see what happens.\n",
        "\n",
        "_Note: Here you can either load up my pretrained Lora adapter from HuggingFace hub.  Or if you trained your own adapter above you can uncomment the specified line below to load your adapter from disk._"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4v2dxcDAuKuH",
        "outputId": "938f9834-e1e5-4a93-d551-8214c535db6f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Peft model adapter loaded\n"
          ]
        }
      ],
      "source": [
        "peft_model_id = f'johnrobinsn/{peft_name}' # By default use my pretrained adapter weights\n",
        "#peft_model_id = peft_name # Uncomment to use locally saved adapter weights if you trained above\n",
        "\n",
        "# Load the LoRA model\n",
        "model = PeftModel.from_pretrained(model, peft_model_id, device_map={\"\":0})\n",
        "model.eval()\n",
        "\n",
        "print(\"Peft model adapter loaded\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dTusTZ-nuKuH"
      },
      "source": [
        "let's try the same prompt again.\n",
        "\n",
        "**AFTER FINETUNING**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ju0ZLTMhuKuH",
        "outputId": "7254000d-9ff2-4451-981c-3869a0d83397"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "John had been thinking about what he should do with his life for a while. He was an ambitious man, but his job responsibilities were limited and he was unsure how to advance his career. He was torn between staying with the company he had worked for for several years or leaving to pursue his own dreams. He had a hard time making the right decision.\n",
            "\n",
            "One day, he was sitting at his desk, thinking. Suddenly, a bright idea came to him. He decided to create a blog about his own ideas and inspirational stories. He was sure he could make this work as a side hustle. He knew it would be difficult to get started, but he was determined to try.\n",
            "\n",
            "He decided to start his blog by setting up a website and putting up his first few posts. He had no idea what to write or how it would turn out. He was nervous, but he knew this was a new beginning and he was ready to try something new. \n",
            "\n",
            "As the weeks went by, he wrote about various subjects and topics. He soon realized that writing and posting on his blog was a great outlet for him. He could talk to people about the ideas and insights he had and it felt good to have something to focus on. \n",
            "\n",
            "One day, he received an unexpected message from someone on his blog. They had read one of his posts and they were so inspired by it that they were inspired to pursue their own ideas. They wanted to connect with John\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(42)\n",
        "generate('Write a short story in third person narration about a protagonist who has to make an important career decision.',maxTokens=300)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ujg1c8lkuKuH"
      },
      "source": [
        "As you can see this response is much much more responsive to the provided instruction."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l27cxX67uKuH"
      },
      "source": [
        "### A Few More Prompts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x-Y85sskuKuH",
        "outputId": "a565ef5c-7e12-4d27-a1c4-c589bc4c43a2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Neil Armstrong was the first man to walk on the moon and he was born in 1930 in Wapakoneta, Ohio.\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(42)\n",
        "generate('Who was the first man to walk on the moon and tell me where he was born.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PRlT7s_cuKuH"
      },
      "source": [
        "In this example, we provide not only an instruction but also provide some context for the instruction which is a list of possible answers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MeXzqkQ7uKuH",
        "outputId": "17fd2167-1dc9-4641-d2db-66a4bfa96b40"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Telegram.\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(42)\n",
        "generate('Identify the odd one out','Twitter, Instagram, Telegram')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JXF_27rJuKuH",
        "outputId": "5b482588-2519-415c-97de-caea569801fa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Soft fur, warm and bright, \n",
            "Cats look so cute and cuddly. \n",
            "Their gentle eyes show their true selves,\n",
            "Filling your heart with warmth and love. \n",
            "\n",
            "Their purring is a lullaby,\n",
            "Calming and soothing you can't ignore. \n",
            "Their laughter is a song of joy, \n",
            "It brings out the happiest of days. \n",
            "\n",
            "Their eyes are as bright as stars,\n",
            "A light that brings joy that no-one can deny. \n",
            "Their purrs are a symphony of sound, \n",
            "Bringing love to every home. \n",
            "\n",
            "So take a moment to pet a cat,\n",
            "It's a purrfect experience, that's for sure. \n",
            "They fill the room with sweet and soothing sound, \n",
            "They bring light and joy in every corner.\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(42)\n",
        "generate('Write a poem about about a cat',maxTokens=1000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WkUjeWORuKuH"
      },
      "source": [
        "Meh.  Not that great... But Llama doesn't seem to be very good at poetry either in my experience.  Would be worthwhile to see how the larger RedPJ models fair here...   Still lot's of fun probing the limits of what works well and what doesn't."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VXCjctlDuKuH"
      },
      "source": [
        "### Conclusion\n",
        "\n",
        "I hope you've enjoyed this quick tour of finetuning.  I've also included a [\"cleaned\" version](https://github.com/johnrobinsn/redpajama/blob/main/finetune_redpajama_clean.ipynb) of this notebook in the github repo without the blog narrative.\n",
        "\n",
        "If you'd like to try your hand at a different finetuning task.  You could give summarization a try.  Please check out the [samsum](https://huggingface.co/datasets/samsum) summarization dataset on HF.  Primarily, you'll need to adjust the prompt templates during training and inference.\n",
        "\n",
        "Please **like** my content on twitter, [@johnrobinsn](https://twitter.com/johnrobinsn)\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "vscode": {
      "interpreter": {
        "hash": "4eddb440959a89d6755541b4ddad18aa547193e63575f384aa5d9491d8be2537"
      }
    },
    "colab": {
      "provenance": [],
      "gpuType": "V100",
      "include_colab_link": true
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "d0508368ab954234be239154b2c49298": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1301d396d7fd4fb0bb5d5f159695025b",
              "IPY_MODEL_6f38370a8b1848e6a0e00f5fb4afe5e0",
              "IPY_MODEL_d225b2606c6c4e45ab72fe87d194c4f2"
            ],
            "layout": "IPY_MODEL_e71b2af50d4f4b3bbc8ebf07c39a1651"
          }
        },
        "1301d396d7fd4fb0bb5d5f159695025b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d60461374273450fb04a95046ec936b9",
            "placeholder": "​",
            "style": "IPY_MODEL_3363b42c37e541a2b3ff03a0563189a6",
            "value": "Map: 100%"
          }
        },
        "6f38370a8b1848e6a0e00f5fb4afe5e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bbc056a3090c428080bba99cb843c699",
            "max": 49942,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0fcdabc3c6634cb3bbd2e090147d341e",
            "value": 49942
          }
        },
        "d225b2606c6c4e45ab72fe87d194c4f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_037a209ff570467f9ce83d224afd873f",
            "placeholder": "​",
            "style": "IPY_MODEL_6b52943dc06442f5be21797cff61900b",
            "value": " 49875/49942 [00:54&lt;00:00, 712.54 examples/s]"
          }
        },
        "e71b2af50d4f4b3bbc8ebf07c39a1651": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": "hidden",
            "width": null
          }
        },
        "d60461374273450fb04a95046ec936b9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3363b42c37e541a2b3ff03a0563189a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bbc056a3090c428080bba99cb843c699": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0fcdabc3c6634cb3bbd2e090147d341e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "037a209ff570467f9ce83d224afd873f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6b52943dc06442f5be21797cff61900b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f98ce1ec9b07466a96fecfe3943baaa1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f96b64fc475c41efb1e86e4ecd7f4ccb",
              "IPY_MODEL_acbf51452aa54d9c90f5cb08f108ca39",
              "IPY_MODEL_ebdd0bac53d7472a95f48c6137fff610"
            ],
            "layout": "IPY_MODEL_9ebe6403e0b14f0997f6b8fcbbb49ba8"
          }
        },
        "f96b64fc475c41efb1e86e4ecd7f4ccb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1b6928fee2674c98a5123b51f886eae9",
            "placeholder": "​",
            "style": "IPY_MODEL_2c749fc7a1f0459aaadfadae75e8887f",
            "value": "Map:  96%"
          }
        },
        "acbf51452aa54d9c90f5cb08f108ca39": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_42027765c2c34069abcf79f5f6df84c7",
            "max": 2000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9a2c3c39ae6840cb86144d81d2cbb7bf",
            "value": 2000
          }
        },
        "ebdd0bac53d7472a95f48c6137fff610": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f0eb2a06cea84b9ab952449e6afdbd70",
            "placeholder": "​",
            "style": "IPY_MODEL_cab4dced8ab040cc9dc008d1fc5ec2b2",
            "value": " 1929/2000 [00:02&lt;00:00, 1049.55 examples/s]"
          }
        },
        "9ebe6403e0b14f0997f6b8fcbbb49ba8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": "hidden",
            "width": null
          }
        },
        "1b6928fee2674c98a5123b51f886eae9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2c749fc7a1f0459aaadfadae75e8887f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "42027765c2c34069abcf79f5f6df84c7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9a2c3c39ae6840cb86144d81d2cbb7bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f0eb2a06cea84b9ab952449e6afdbd70": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cab4dced8ab040cc9dc008d1fc5ec2b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}