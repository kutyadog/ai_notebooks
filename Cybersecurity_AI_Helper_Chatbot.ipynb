{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kutyadog/ai_notebooks/blob/main/Cybersecurity_AI_Helper_Chatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Install necessary libraries\n",
        "# This cell needs to be run first to ensure all dependencies are available.\n",
        "print(\"Installing required libraries...\")\n",
        "!pip install -q groq transformers torch\n",
        "print(\"Libraries installed.\")"
      ],
      "metadata": {
        "id": "IOULLGYBZJb0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "This Colab notebook provides a proof-of-concept for integrating a Groq AI chatbot\n",
        "with a local cybersecurity base transformer model (fdtn-ai/Foundation-Sec-8B).\n",
        "\n",
        "The chatbot will primarily use the Groq AI model for general conversation.\n",
        "However, if the user's query is identified as cybersecurity-related,\n",
        "the query will be routed to the Foundation-Sec-8B model for a specialized response.\n",
        "\n",
        "This code is designed to be run in a Google Colab environment, ideally with a GPU\n",
        "runtime for faster inference with the Foundation-Sec-8B model.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "# 2. Set up your GROQ_API_KEY\n",
        "# IMPORTANT: Replace 'YOUR_GROQ_API_KEY' with your actual Groq API key.\n",
        "# For better security, consider storing this in Colab Secrets or environment variables.\n",
        "# Example:\n",
        "# from google.colab import userdata\n",
        "# GROQ_API_KEY = userdata.get('GROQ_API_KEY')\n",
        "# If you don't have it in secrets, you can paste it directly here for a quick test:\n",
        "import os\n",
        "GROQ_API_KEY = os.environ.get(\"GROQ_API_KEY\", \"YOUR_GROQ_API_KEY\") # Replace with your actual key or set as environment variable\n",
        "\n",
        "if GROQ_API_KEY == \"YOUR_GROQ_API_KEY\":\n",
        "    print(\"\\nWARNING: Please replace 'YOUR_GROQ_API_KEY' with your actual Groq API Key.\")\n",
        "    print(\"You can get one from: https://console.groq.com/keys\")\n",
        "    print(\"Alternatively, set it as a Colab Secret named 'GROQ_API_KEY'.\")\n",
        "    # Exit or prompt for key if not set, for this PoC we'll proceed but it will fail if key is invalid.\n",
        "\n",
        "# 3. Import and initialize Groq client\n",
        "from groq import Groq\n",
        "from google.colab import userdata\n",
        "\n",
        "\n",
        "try:\n",
        "    groq_client = Groq(api_key=userdata.get('GROQ_API_KEY'))\n",
        "    print(\"\\nGroq client initialized.\")\n",
        "except Exception as e:\n",
        "    print(f\"\\nError initializing Groq client: {e}\")\n",
        "    print(\"Please ensure your GROQ_API_KEY is correct and valid.\")\n",
        "    groq_client = None # Set to None if initialization fails\n",
        "\n",
        "# 4. Load the Cybersecurity Transformer Model\n",
        "# This might take a few minutes depending on your Colab runtime and internet speed.\n",
        "print(\"\\nLoading Foundation-Sec-8B model and tokenizer...\")\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "try:\n",
        "    # Attempt to load the model in half-precision (float16) to reduce RAM usage.\n",
        "    # This is often sufficient to fit larger models into Colab's free tier RAM.\n",
        "    cybersecurity_tokenizer = AutoTokenizer.from_pretrained(\"fdtn-ai/Foundation-Sec-8B\")\n",
        "    cybersecurity_model = AutoModelForCausalLM.from_pretrained(\n",
        "        \"fdtn-ai/Foundation-Sec-8B\",\n",
        "        torch_dtype=torch.float16 # Load in half-precision\n",
        "    )\n",
        "    # Move model to GPU if available for faster inference\n",
        "    if torch.cuda.is_available():\n",
        "        cybersecurity_model.to(\"cuda\")\n",
        "        print(\"Foundation-Sec-8B model moved to GPU.\")\n",
        "    else:\n",
        "        print(\"CUDA not available, Foundation-Sec-8B model will run on CPU (may be slower).\")\n",
        "    print(\"Foundation-Sec-8B model loaded successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading Foundation-Sec-8B model: {e}\")\n",
        "    print(\"Please check the model path, your internet connection, and available RAM.\")\n",
        "    print(\"If the issue persists, consider a Colab Pro subscription or using a smaller model.\")\n",
        "    cybersecurity_model = None\n",
        "    cybersecurity_tokenizer = None\n",
        "\n",
        "# 5. Define a function to interact with the Cybersecurity Model\n",
        "def get_cybersecurity_response(prompt: str) -> str:\n",
        "    \"\"\"\n",
        "    Generates a response using the Foundation-Sec-8B model for cybersecurity queries.\n",
        "    \"\"\"\n",
        "    if cybersecurity_model is None or cybersecurity_tokenizer is None:\n",
        "        return \"Cybersecurity model is not loaded or initialized. Cannot process this request.\"\n",
        "\n",
        "    # For this PoC, we'll try to frame the prompt to match the example given.\n",
        "    # In a real application, you'd need more sophisticated prompt engineering\n",
        "    # or fine-tuning for various cybersecurity tasks.\n",
        "    formatted_prompt = f\"Analyze the following cybersecurity context: {prompt}\\n\\nProvide relevant cybersecurity insights or information.\"\n",
        "\n",
        "    try:\n",
        "        inputs = cybersecurity_tokenizer(formatted_prompt, return_tensors=\"pt\")\n",
        "        if torch.cuda.is_available():\n",
        "            inputs = {k: v.to(\"cuda\") for k, v in inputs.items()} # Move inputs to GPU\n",
        "\n",
        "        outputs = cybersecurity_model.generate(\n",
        "            inputs[\"input_ids\"],\n",
        "            max_new_tokens=100, # Increased tokens for potentially more detailed responses\n",
        "            do_sample=True,\n",
        "            temperature=0.7, # Adjusted for more diverse but still coherent output\n",
        "            top_p=0.9,\n",
        "            pad_token_id=cybersecurity_tokenizer.eos_token_id # Important for generation\n",
        "        )\n",
        "\n",
        "        response = cybersecurity_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        # Remove the original prompt from the response\n",
        "        response = response.replace(formatted_prompt, \"\").strip()\n",
        "        return response\n",
        "    except Exception as e:\n",
        "        return f\"Error generating response from cybersecurity model: {e}\"\n",
        "\n",
        "# 6. Define a function to interact with the Groq AI Model\n",
        "def get_groq_response(prompt: str, chat_history: list) -> str:\n",
        "    \"\"\"\n",
        "    Generates a response using the Groq AI model for general queries.\n",
        "    \"\"\"\n",
        "    if groq_client is None:\n",
        "        return \"Groq AI client is not initialized. Cannot process this request.\"\n",
        "\n",
        "    messages = chat_history + [{\"role\": \"user\", \"content\": prompt}]\n",
        "    try:\n",
        "        chat_completion = groq_client.chat.completions.create(\n",
        "            messages=messages,\n",
        "            model=\"llama3-8b-8192\", # Or \"mixtral-8x7b-32768\" or \"gemma-7b-it\"\n",
        "            temperature=0.7,\n",
        "            max_tokens=1024,\n",
        "            top_p=1,\n",
        "            stop=None,\n",
        "            stream=False,\n",
        "        )\n",
        "        return chat_completion.choices[0].message.content\n",
        "    except Exception as e:\n",
        "        return f\"Error generating response from Groq AI: {e}\"\n",
        "\n",
        "# 7. Main Chatbot Loop\n",
        "def main_chatbot():\n",
        "    print(\"\\n--- Groq AI Chatbot with Cybersecurity Integration ---\")\n",
        "    print(\"Type 'exit' or 'quit' to end the conversation.\")\n",
        "    print(\"Try asking cybersecurity-related questions (e.g., 'What is CVE-2021-44228?', 'Explain MITRE ATT&CK').\")\n",
        "    print(\"For general questions, the Groq AI will respond.\")\n",
        "\n",
        "    chat_history = []\n",
        "\n",
        "    while True:\n",
        "        user_input = input(\"\\nYou: \").strip()\n",
        "        if user_input.lower() in [\"exit\", \"quit\"]:\n",
        "            print(\"Chatbot: Goodbye!\")\n",
        "            break\n",
        "\n",
        "        # Simple keyword-based detection for cybersecurity queries\n",
        "        # This can be made much more sophisticated using NLP techniques or even\n",
        "        # by asking the Groq model itself to classify the query.\n",
        "        cybersecurity_keywords = [\n",
        "            \"cve\", \"cwe\", \"vulnerability\", \"exploit\", \"malware\", \"phishing\",\n",
        "            \"ransomware\", \"firewall\", \"ids\", \"ips\", \"soc\", \"threat intelligence\",\n",
        "            \"incident response\", \"mitre att&ck\", \"zero-day\", \"security posture\",\n",
        "            \"compliance\", \"penetration test\", \"red team\", \"blue team\", \"cyberattack\"\n",
        "        ]\n",
        "        is_cybersecurity_query = any(keyword in user_input.lower() for keyword in cybersecurity_keywords)\n",
        "\n",
        "        response = \"\"\n",
        "        if is_cybersecurity_query:\n",
        "            print(\"Chatbot (Cybersecurity Model): Thinking...\")\n",
        "            response = get_cybersecurity_response(user_input)\n",
        "        else:\n",
        "            print(\"Chatbot (Groq AI): Thinking...\")\n",
        "            response = get_groq_response(user_input, chat_history)\n",
        "\n",
        "        print(f\"Chatbot: {response}\")\n",
        "        chat_history.append({\"role\": \"user\", \"content\": user_input})\n",
        "        chat_history.append({\"role\": \"assistant\", \"content\": response})\n",
        "\n",
        "# Run the chatbot\n",
        "if __name__ == \"__main__\":\n",
        "    main_chatbot()\n",
        "\n"
      ],
      "metadata": {
        "id": "bGqmPDfKWOse"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run these commands in a Colab cell to install necessary libraries for TPU.\n",
        "# IMPORTANT: After running this cell, go to Runtime -> Restart runtime,\n",
        "# then proceed to the next cells. This is crucial for torch_xla to initialize correctly.\n",
        "\n",
        "# 1. Aggressively uninstall potentially conflicting pre-installed packages.\n",
        "#    This ensures a clean slate for TPU-specific installations.\n",
        "#    Added 'thinc' to the uninstall list to resolve NumPy 2.x conflict.\n",
        "!pip uninstall -y fastai torchaudio torchvision torch accelerate transformers numpy thinc\n",
        "\n",
        "# 2. Install a compatible version of NumPy (less than 2.0).\n",
        "!pip install -q numpy==1.26.4 # Explicitly install a NumPy 1.x version\n",
        "\n",
        "# 3. Install PyTorch and TPU-specific libraries.\n",
        "#    Note: We are not using CUDA-specific PyTorch here, as we are on TPU.\n",
        "!pip install -q torch==2.2.1 torchvision==0.17.1 torchaudio==2.2.1\n",
        "!pip install -q cloud-tpu-client==0.10 torch_xla==2.2\n",
        "\n",
        "# 4. Install other required libraries.\n",
        "!pip install -q accelerate transformers requests\n",
        "\n"
      ],
      "metadata": {
        "id": "biXFQa3EAkPG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y thinc\n",
        "\n",
        "# 2. Install a compatible version of NumPy (less than 2.0).\n",
        "# !pip install -q scipy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x4b6tdOYQKOl",
        "outputId": "f2c2ff50-ab48-4e06-d875-4a032add0986"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: thinc 8.3.6\n",
            "Uninstalling thinc-8.3.6:\n",
            "  Successfully uninstalled thinc-8.3.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Colab Setup and Library Installations (for TPU) ---\n",
        "\n",
        "\n",
        "# --- Imports ---\n",
        "import json\n",
        "import asyncio\n",
        "import requests # This is needed for making HTTP requests to the GROQ API\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch # For general PyTorch operations\n",
        "import torch_xla.core.xla_model as xm # For TPU device management\n",
        "from google.colab import userdata\n",
        "\n",
        "\n",
        "# --- Global Variables for Model and Tokenizer ---\n",
        "# These will be loaded once when the script starts in Colab\n",
        "foundation_sec_model = None\n",
        "foundation_sec_tokenizer = None\n",
        "# Set DEVICE to TPU\n",
        "DEVICE = xm.xla_device() if 'xla' in str(xm.xla_device()) else \"cpu\"\n",
        "\n",
        "\n",
        "# --- GROQ API Configuration ---\n",
        "# IMPORTANT: Replace \"YOUR_GROQ_API_KEY\" with your actual GROQ API key.\n",
        "# You can get one from https://console.groq.com/keys\n",
        "GROQ_API_KEY = userdata.get('GROQ_API_KEY')\n",
        "GROQ_CHAT_MODEL = \"llama3-8b-8192\" # A capable and cheap model from GROQ\n",
        "\n",
        "# --- Model Loading Function ---\n",
        "async def load_foundation_sec_model():\n",
        "    \"\"\"\n",
        "    Loads the Foundation-Sec-8B model and tokenizer for TPU.\n",
        "    This function should be called once at the start of your Colab session.\n",
        "    \"\"\"\n",
        "    global foundation_sec_model, foundation_sec_tokenizer\n",
        "    if foundation_sec_model is None:\n",
        "        print(\"Loading fdtn-ai/Foundation-Sec-8B model and tokenizer for TPU...\")\n",
        "\n",
        "        # Check if TPU is available\n",
        "        if 'xla' not in str(DEVICE):\n",
        "            print(\"ERROR: TPU is not available. Please ensure you have a TPU runtime enabled in Colab.\")\n",
        "            print(\"Go to Runtime -> Change runtime type -> Hardware accelerator -> TPU.\")\n",
        "            return\n",
        "\n",
        "        try:\n",
        "            foundation_sec_tokenizer = AutoTokenizer.from_pretrained(\"fdtn-ai/Foundation-Sec-8B\")\n",
        "            # Load the model directly to the TPU device.\n",
        "            # 4-bit quantization (bitsandbytes) is not used with TPUs.\n",
        "            foundation_sec_model = AutoModelForCausalLM.from_pretrained(\n",
        "                \"fdtn-ai/Foundation-Sec-8B\",\n",
        "                torch_dtype=torch.float16, # Use float16 for memory efficiency on TPU\n",
        "            )\n",
        "            # Move model to TPU device\n",
        "            foundation_sec_model.to(DEVICE)\n",
        "            foundation_sec_model.eval() # Set model to evaluation mode\n",
        "            print(\"Model loaded successfully!\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading Foundation-Sec-8B model: {e}\")\n",
        "            print(\"Please ensure you have a TPU runtime enabled in Colab (Runtime -> Change runtime type).\")\n",
        "            print(\"If issues persist, try restarting the Colab runtime or checking TPU memory usage.\")\n",
        "            # Fallback or error handling if model loading fails\n",
        "            foundation_sec_model = None\n",
        "            foundation_sec_tokenizer = None\n",
        "\n",
        "# --- Foundation-Sec-8B Inference Function ---\n",
        "async def generate_cyber_response(prompt: str, max_new_tokens: int = 200) -> str:\n",
        "    \"\"\"\n",
        "    Generates a response using the loaded Foundation-Sec-8B model on TPU.\n",
        "\n",
        "    Args:\n",
        "        prompt (str): The input prompt for the model.\n",
        "        max_new_tokens (int): The maximum number of tokens to generate.\n",
        "\n",
        "    Returns:\n",
        "        str: The generated text from the Foundation-Sec-8B model.\n",
        "    \"\"\"\n",
        "    global foundation_sec_model, foundation_sec_tokenizer\n",
        "\n",
        "    if foundation_sec_model is None or foundation_sec_tokenizer is None:\n",
        "        print(\"Foundation-Sec-8B model not loaded. Attempting to load now...\")\n",
        "        await load_foundation_sec_model()\n",
        "        if foundation_sec_model is None:\n",
        "            return \"Error: Cybersecurity model not available. Please ensure it loaded correctly.\"\n",
        "\n",
        "    try:\n",
        "        # Encode the prompt and move inputs to the TPU device\n",
        "        inputs = foundation_sec_tokenizer(prompt, return_tensors=\"pt\").to(DEVICE)\n",
        "\n",
        "        # Generate response\n",
        "        with torch.no_grad(): # Disable gradient calculation for inference\n",
        "            outputs = foundation_sec_model.generate(\n",
        "                inputs[\"input_ids\"], # Use input_ids directly as in the example\n",
        "                max_new_tokens=max_new_tokens,\n",
        "                num_return_sequences=1,\n",
        "                pad_token_id=foundation_sec_tokenizer.eos_token_id, # Good practice for generation\n",
        "                do_sample=True, # Enable sampling for more diverse responses\n",
        "                temperature=0.1, # Adjusted to match example\n",
        "                top_k=50, # Retained for control, example didn't specify but often used with do_sample\n",
        "                top_p=0.9, # Adjusted to match example\n",
        "            )\n",
        "\n",
        "        # Decode the generated tokens\n",
        "        generated_text = foundation_sec_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "        # The generated_text will include the prompt, so we need to remove it\n",
        "        # A simple way is to find the prompt in the generated text and return what comes after.\n",
        "        # This might need refinement based on how the model typically responds.\n",
        "        if generated_text.startswith(prompt):\n",
        "            return generated_text[len(prompt):].strip()\n",
        "        return generated_text.strip()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error generating response from Foundation-Sec-8B: {e}\")\n",
        "        return \"An error occurred while getting cybersecurity information. Check model loading and TPU.\"\n",
        "\n",
        "# --- Updated get_cybersecurity_info to use Foundation-Sec-8B ---\n",
        "async def get_cybersecurity_info(query: str) -> str:\n",
        "    \"\"\"\n",
        "    Queries the Foundation-Sec-8B model for cybersecurity context.\n",
        "\n",
        "    Args:\n",
        "        query (str): The user's query related to cybersecurity.\n",
        "\n",
        "    Returns:\n",
        "        str: The response from the Foundation-Sec-8B model, acting as cybersecurity context.\n",
        "    \"\"\"\n",
        "    print(f\"Querying Foundation-Sec-8B for cybersecurity context: '{query}'\")\n",
        "    # Construct a prompt suitable for the Foundation-Sec-8B model\n",
        "    # The model is a base model, so it needs a clear instruction.\n",
        "    cyber_prompt = f\"Provide detailed information about the following cybersecurity topic: {query}\\n\\nAnswer:\"\n",
        "    cyber_context = await generate_cyber_response(cyber_prompt)\n",
        "\n",
        "    if cyber_context:\n",
        "        return cyber_context\n",
        "    else:\n",
        "        return \"No specific cybersecurity context found from Foundation-Sec-8B for this query.\"\n",
        "\n",
        "# --- Updated chat_with_cyber_helper to use GROQ API ---\n",
        "async def chat_with_cyber_helper(user_message: str) -> str:\n",
        "    \"\"\"\n",
        "    Acts as the main chatbot logic, integrating general chat capabilities\n",
        "    with specialized cybersecurity knowledge from Foundation-Sec-8B,\n",
        "    and using the GROQ API for general chat responses.\n",
        "\n",
        "    Args:\n",
        "        user_message (str): The message from the user.\n",
        "\n",
        "    Returns:\n",
        "        str: The chatbot's response.\n",
        "    \"\"\"\n",
        "    # Define keywords to identify cybersecurity-related queries\n",
        "    cyber_keywords = [\n",
        "        'cybersecurity', 'vulnerability', 'threat', 'malware', 'exploit',\n",
        "        'phishing', 'ransomware', 'firewall', 'encryption', 'security',\n",
        "        'attack', 'breach', 'incident response', 'SOC', 'TTP', 'compliance',\n",
        "        'patch', 'zero-day', 'DDos', 'APT', 'SIEM', 'IDS', 'IPS', 'VPN',\n",
        "        'authentication', 'authorization', 'audit', 'forensics', 'risk assessment',\n",
        "        'cyber attack', 'data breach', 'security policy', 'penetration testing',\n",
        "        'ethical hacking', 'threat intelligence', 'vulnerability management'\n",
        "    ]\n",
        "\n",
        "    is_cyber_related = any(keyword in user_message.lower() for keyword in cyber_keywords)\n",
        "\n",
        "    system_prompt = \"\"\n",
        "    user_content = \"\"\n",
        "\n",
        "    if is_cyber_related:\n",
        "        # If the query is cybersecurity-related, get context from Foundation-Sec-8B\n",
        "        cyber_context = await get_cybersecurity_info(user_message)\n",
        "\n",
        "        # Construct a detailed prompt for the GROQ chat model\n",
        "        system_prompt = f\"\"\"\n",
        "        You are a helpful and knowledgeable AI assistant specializing in cybersecurity.\n",
        "        Your goal is to provide accurate and relevant information based on the user's query.\n",
        "\n",
        "        Here is some cybersecurity context that might be relevant to the user's question,\n",
        "        generated by a specialized cybersecurity model:\n",
        "        ---\n",
        "        {cyber_context}\n",
        "        ---\n",
        "\n",
        "        Please use this context to inform your answer. If the context is not directly relevant or is insufficient,\n",
        "        please answer based on your general knowledge about cybersecurity.\n",
        "        \"\"\"\n",
        "        user_content = user_message\n",
        "    else:\n",
        "        # If not cybersecurity-related, use a general prompt for the GROQ model\n",
        "        system_prompt = \"You are a helpful AI assistant. Answer the user's question.\"\n",
        "        user_content = user_message\n",
        "\n",
        "    # Prepare the messages for the GROQ API\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": system_prompt},\n",
        "        {\"role\": \"user\", \"content\": user_content}\n",
        "    ]\n",
        "\n",
        "    # Call the GROQ API to generate the response\n",
        "    api_url = \"https://api.groq.com/openai/v1/chat/completions\"\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {GROQ_API_KEY}\",\n",
        "        \"Content-Type\": \"application/json\"\n",
        "    }\n",
        "    payload = {\n",
        "        \"model\": GROQ_CHAT_MODEL,\n",
        "        \"messages\": messages,\n",
        "        \"temperature\": 0.7,\n",
        "        \"max_tokens\": 500 # Adjust as needed\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        print(f\"Calling GROQ API with model '{GROQ_CHAT_MODEL}' for general chat response...\")\n",
        "        response = requests.post(api_url, headers=headers, json=payload)\n",
        "        response.raise_for_status() # Raise an exception for HTTP errors (4xx or 5xx)\n",
        "        result = response.json()\n",
        "\n",
        "        if result.get(\"choices\") and result[\"choices\"][0].get(\"message\") and \\\n",
        "           result[\"choices\"][0][\"message\"].get(\"content\"):\n",
        "            return result[\"choices\"][0][\"message\"][\"content\"]\n",
        "        else:\n",
        "            print(f\"Unexpected GROQ API response structure: {result}\")\n",
        "            return \"I couldn't generate a response based on your query. The API response was unexpected.\"\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Error calling GROQ API: {e}\")\n",
        "        return \"I apologize, but I encountered an error while trying to process your request to the general AI.\"\n",
        "    except json.JSONDecodeError as e:\n",
        "        print(f\"Error decoding JSON from GROQ API response: {e}\")\n",
        "        return \"I apologize, but I received an unreadable response from the general AI.\"\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred: {e}\")\n",
        "        return \"An unexpected error occurred while processing your request.\"\n",
        "\n",
        "\n",
        "async def main():\n",
        "    \"\"\"\n",
        "    Main function to run the chatbot interaction loop.\n",
        "    Includes KeyboardInterrupt handling for graceful exit in Colab.\n",
        "    \"\"\"\n",
        "    print(\"Welcome to the Cybersecurity AI Helper!\")\n",
        "    print(\"Initializing models. This might take a moment...\")\n",
        "    await load_foundation_sec_model() # Load the Foundation-Sec-8B model at startup\n",
        "\n",
        "    if foundation_sec_model is None:\n",
        "        print(\"Failed to load cybersecurity model. Chatbot will operate in general mode.\")\n",
        "\n",
        "    print(\"\\nType 'exit' to end the chat.\")\n",
        "    print(\"Press Ctrl+C or use Colab's 'Stop' button to gracefully interrupt.\")\n",
        "\n",
        "    try:\n",
        "        while True:\n",
        "            user_input = input(\"\\nYou: \")\n",
        "            if user_input.lower() == 'exit':\n",
        "                print(\"Goodbye!\")\n",
        "                break\n",
        "\n",
        "            response = await chat_with_cyber_helper(user_input)\n",
        "            print(f\"Cyber Helper: {response}\")\n",
        "    except KeyboardInterrupt:\n",
        "        print(\"\\nChatbot interrupted by user. Exiting gracefully.\")\n",
        "    except Exception as e:\n",
        "        print(f\"\\nAn unexpected error occurred during chat: {e}\")\n",
        "    finally:\n",
        "        print(\"Chat session ended.\")\n",
        "\n",
        "\n",
        "# To run this in Google Colab, execute the cells sequentially:\n",
        "# 1. The pip install commands.\n",
        "# 2. The rest of the code.\n",
        "# 3. Call asyncio.run(main()) in a new cell.\n",
        "if __name__ == \"__main__\":\n",
        "    # In a Colab environment, you would typically run this in a separate cell\n",
        "    # after defining all functions.\n",
        "    # asyncio.run(main())\n",
        "    print(\"\\nTo start the chat in Colab, run 'await main()' in a new cell after executing this one.\")\n",
        "    print(\"Remember to replace 'YOUR_GROQ_API_KEY' with your actual key.\")\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "GIlFy4rx_3zS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "await main()"
      ],
      "metadata": {
        "id": "StvdnqCNEA7y"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}