{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyNnIk1NaTWtxpBbmYDjoXYH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kutyadog/ai_notebooks/blob/main/RouteLLM_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aug 2024"
      ],
      "metadata": {
        "id": "I5nvPkPW12PB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aPJhyOGw2PWU"
      },
      "outputs": [],
      "source": [
        "!pip install \"routellm[serve,eval]\"\n",
        "!pip install gradio\n",
        "!git clone https://github.com/open-webui/open-webui.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !docker run -d -p 3000:8080 --add-host=host.docker.internal:host-gateway -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:main\n",
        "!udocker --allow-root run -d -p 3000:8080 --add-host=host.docker.internal:host-gateway -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:main"
      ],
      "metadata": {
        "id": "vNOAE3W5gTeX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "pip install udocker\n",
        "udocker --allow-root install"
      ],
      "metadata": {
        "id": "d7k1hy4BhA22"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !docker-init\n",
        "!apt-get -qq install docker.io"
      ],
      "metadata": {
        "id": "1rTLAnhggiw8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "# Set the environment variable before importing the OpenAI library\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')\n",
        "# Replace with your model provider, we use Anyscale's Mixtral here.\n",
        "# os.environ[\"ANYSCALE_API_KEY\"] = userdata.get('ANYSCALE_API_KEY')\n",
        "\n",
        "from routellm.controller import Controller\n",
        "\n",
        "client = Controller(\n",
        "  routers=[\"mf\"],\n",
        "  strong_model=\"gpt-4-1106-preview\",\n",
        "  weak_model=\"anyscale/mistralai/Mixtral-8x7B-Instruct-v0.1\",\n",
        ")"
      ],
      "metadata": {
        "id": "6Mxf93_Y2bMt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m routellm.calibrate_threshold --routers mf --strong-model-pct 0.5 --config config.example.yaml\n",
        "# For 50.0% strong model calls for mf, threshold = 0.11593\n",
        "# !python -m routellm.openai_server --routers mf --strong-model gpt-4-1106-preview --weak-model anyscale/mistralai/Mixtral-8x7B-Instruct-v0.1\n",
        "# !python -m examples.router_chat --router mf --threshold 0.11593"
      ],
      "metadata": {
        "id": "rl_IWGOu5-WU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Gradio interface for testing routes\n",
        "\n",
        "import re\n",
        "import gradio as gr\n",
        "from routellm.controller import Controller\n",
        "\n",
        "TEMPERATURE = 0.8\n",
        "THRESHOLD = 0.11593\n",
        "ROUTER = \"mf\"\n",
        "\n",
        "os.environ[\"GROQ_API_KEY\"] = userdata.get('GROQ_API_KEY')\n",
        "\n",
        "client = Controller(\n",
        "  routers=[\"mf\"],\n",
        "  strong_model=\"gpt-4-1106-preview\",\n",
        "  weak_model=\"groq/llama3-8b-8192\"\n",
        ")\n",
        "\n",
        "\n",
        "# Notes:\n",
        "\n",
        "# https://github.com/bnurbekov/LLM-Agents/blob/6118dcc59605e88030551b1a543bab66d6bb2416/route_llm/agent.py\n",
        "\n",
        "# good examples of chat with rag, memory, etc.\n",
        "# https://rito.hashnode.dev/building-rag-in-2024-with-langchain-groq-llama3-and-qdrant\n",
        "\n",
        "# https://huggingface.co/spaces/routellm/demo/blob/main/app.py\n",
        "# https://github.com/ralphbutler/LLM_misc/blob/36b470103ae1aa88d75e90c3ff1e4d7e9fd48674/routellm_demo1.py (wasnt working last check)\n",
        "# https://www.youtube.com/watch?v=jc2RCG1Ys7g\n",
        "\n",
        "# python embeddings, etc.\n",
        "# https://www.linkedin.com/pulse/write-query-engine-olm-stack-groq-krishna-tripathi-ulj6f/\n",
        "#\n",
        "\n",
        "def predict(message, history, threshold, temperature):\n",
        "    # Convert chat history to OpenAI format\n",
        "    history_openai_format = [\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful AI assistant.\"}\n",
        "    ]\n",
        "    for human, assistant in history:\n",
        "        history_openai_format.append({\"role\": \"user\", \"content\": human})\n",
        "        history_openai_format.append(\n",
        "            {\n",
        "                \"role\": \"assistant\",\n",
        "                \"content\": re.sub(r\"^\\*\\*\\[.*?\\]\\*\\*\\s*\", \"\", assistant),\n",
        "            }\n",
        "        )\n",
        "    history_openai_format.append({\"role\": \"user\", \"content\": message})\n",
        "\n",
        "    # Create a chat completion request and send it to the API server\n",
        "    stream = client.chat.completions.create(\n",
        "        model=f\"router-{ROUTER}-{threshold}\",  # Model name to use\n",
        "        messages=history_openai_format,  # Chat history\n",
        "        temperature=temperature,  # Temperature for text generation\n",
        "        stream=True,  # Stream response\n",
        "        max_tokens=512\n",
        "    )\n",
        "    print(stream)\n",
        "\n",
        "    # Read and return generated text from response stream\n",
        "    partial_message = \"\"\n",
        "    for i, chunk in enumerate(stream):\n",
        "        print(chunk)\n",
        "        if i == 0:\n",
        "            model_name = chunk.model\n",
        "            model_prefix = f\"**[{model_name}]**\\n\"\n",
        "            yield model_prefix\n",
        "            partial_message += model_prefix\n",
        "        partial_message += chunk.choices[0].delta.content or \"\"\n",
        "        yield partial_message\n",
        "\n",
        "\n",
        "# Create and launch a chat interface with Gradio\n",
        "demo = gr.ChatInterface(\n",
        "    predict,\n",
        "    additional_inputs=[\n",
        "        gr.Slider(label=\"Threshold\", minimum=0, maximum=1, value=THRESHOLD, step=0.01),\n",
        "        gr.Slider(\n",
        "            label=\"Temperature\", minimum=0, maximum=1, value=TEMPERATURE, step=0.1\n",
        "        ),\n",
        "    ],\n",
        "    title=\"RouteLLM\",\n",
        "    fill_height=True,\n",
        "    description=\"This is a demo of our matrix factorization router, calibrated so that approximately 50% of calls (those that are harder) are routed to GPT-4, with remaining calls routed to Mixtral 8x7B.\\n\\nCheck out https://github.com/lm-sys/RouteLLM for details!\",\n",
        ")\n",
        "\n",
        "demo.launch(debug=True)\n",
        "\n"
      ],
      "metadata": {
        "id": "JC9dxi0StDES",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "76f5051f-06a9-4417-88a5-ba77b0bd69d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://a81c8a9bb4de6a6227.gradio.live\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title simple example to send message, get output and which model was used\n",
        "\n",
        "import os\n",
        "\n",
        "from routellm.controller import Controller\n",
        "\n",
        "os.environ[\"GROQ_API_KEY\"] = userdata.get('GROQ_API_KEY')\n",
        "\n",
        "client = Controller(\n",
        "  routers=[\"mf\"],\n",
        "  strong_model=\"gpt-4-1106-preview\",\n",
        "  weak_model=\"groq/llama3-8b-8192\"\n",
        ")\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "  # This tells RouteLLM to use the MF router with a cost threshold of 0.11593\n",
        "  model=\"router-mf-0.11593\",\n",
        "  messages=[\n",
        "    {\"role\": \"user\", \"content\": \"hello there! I am Chris.\"}\n",
        "  ]\n",
        ")\n",
        "\n",
        "message_content = response['choices'][0]['message']['content']\n",
        "model_name = response['model']\n",
        "\n",
        "print(f\"Message content: {message_content}\")\n",
        "print(f\"Model name: {model_name}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z0FAMzegxU4b",
        "outputId": "2dea4d4a-a546-4143-ca6f-fdddf0790288"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Message content: Hello Chris! It's nice to meet you! Is there something I can help you with or would you like to chat?\n",
            "Model name: groq/llama3-8b-8192\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Trying open webui on colab\n",
        "\n",
        "This works."
      ],
      "metadata": {
        "id": "aoyb2kdplR72"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import urllib\n",
        "print(\"Password/Enpoint IP for localtunnel is:\",urllib.request.urlopen('https://ipv4.icanhazip.com').read().decode('utf8').strip(\"\\n\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GUOeYzVvk9lr",
        "outputId": "3cda4eb4-73f6-4686-9476-c0b3480fbb57"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Password/Enpoint IP for localtunnel is: 35.229.207.30\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!npm install localtunnel\n",
        "!curl -fsSL https://ollama.com/install.sh | sh"
      ],
      "metadata": {
        "id": "x4PuHwxUlV_d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt-get update\n",
        "!sudo apt-get install -y python3.11 python3.11-venv python3.11-dev\n",
        "\n",
        "# Create and activate a virtual environment using Python 3.11\n",
        "!python3.11 -m venv venv\n",
        "!source venv/bin/activate\n",
        "\n",
        "# Upgrade pip within the virtual environment\n",
        "!venv/bin/python -m pip install --upgrade pip\n",
        "\n",
        "# Install Open WebUI within the virtual environment\n",
        "!venv/bin/pip install open-webui\n",
        "\n",
        "# Create a script to start both servers asynchronously and expose them using localtunnel\n",
        "with open('start_servers.py', 'w') as f:\n",
        "    f.write('''\n",
        "import subprocess\n",
        "import threading\n",
        "import os\n",
        "import time\n",
        "\n",
        "def start_ollama():\n",
        "    subprocess.run(['ollama', 'serve'])\n",
        "\n",
        "def download_model():\n",
        "    subprocess.run(['ollama', 'pull', 'mistral-nemo'])\n",
        "\n",
        "def start_open_webui():\n",
        "    subprocess.run(['venv/bin/open-webui', 'serve', '--port', '8081'])\n",
        "\n",
        "# Start servers in separate threads\n",
        "threading.Thread(target=start_ollama).start()\n",
        "time.sleep(5)\n",
        "threading.Thread(target=download_model).start()\n",
        "threading.Thread(target=start_open_webui).start()\n",
        "''')\n",
        "\n",
        "# Execute the script\n",
        "!venv/bin/python start_servers.py && sleep 20 & npx localtunnel --port 8081"
      ],
      "metadata": {
        "id": "FbUEe6-3laVR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}