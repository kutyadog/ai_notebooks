{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kutyadog/ai_notebooks/blob/main/Whisper_voices_cj.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Google drive files transcribing with OPENAI Whisper\n",
        "\n",
        "**Note: - ONLY WORKS WITH AUDIO THAT HAS MONO TYPE**\n",
        "\n",
        "CJ LINKS:\n",
        "\n",
        "https://colab.research.google.com/drive/1HuvcY4tkTHPDzcwyVH77LCh_m8tP-Qet?usp=sharing#scrollTo=jKG14DGYbwku\n",
        "\n",
        "\n",
        "STEPS:\n",
        "\n",
        "1/ Get Google Colab resources (https://colab.research.google.com/signup) [100 points is more than enough for 40+ transcriptions (needed for large-v2 modal)]\n",
        "\n",
        "2/ Write file location path & File name (plus filetype) in variable 'path'.\n",
        "\n",
        "3/ Set Number of speakers, Language & model\n",
        "\n",
        "4/ Run code [under options 'Runtime', select 'Run all'.] [You can find 'Runtime' at the top of the screen, next to 'Insert' & 'Tools']\n",
        "\n",
        "5/ Download .txt file from left menu option [the one with the folder icon] named; 'Files'.\n",
        "\n",
        "\n",
        "How to videos:\n",
        "  1. Getting started video [Not necessary to run code] : https://youtu.be/yVLhG4-7Sj4\n",
        "  2. Audacity set MONO audio: https://www.youtube.com/watch?v=TTbBibBDGpg&ab_channel=LearnAudacity\n",
        "\n",
        "*Note: This requires giving the application permission to connect to your drive. Only you will have access to the contents of your drive, but please read the warnings carefully.*\n",
        "\n",
        "###**For faster performance set your runtime to \"GPU\"**\n",
        "*Click on \"Runtime\" in the menu and click \"Change runtime type\". Select \"GPU\".*"
      ],
      "metadata": {
        "id": "g_A7yaKV-xQT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "# Write file drive location & filename below!\n",
        "path = '/content/slip.wav'\n"
      ],
      "metadata": {
        "id": "Pq7_nz_2xkbW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "16c4a327-3c7b-475e-d1c9-5732e42dac2f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ap_Oozugv3t7"
      },
      "outputs": [],
      "source": [
        "!pip install git+https://github.com/openai/whisper.git\n",
        "!pip install -q git+https://github.com/pyannote/pyannote-audio > /dev/null\n",
        "!sudo apt update && sudo apt install ffmpeg"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import whisper\n",
        "import datetime\n",
        "\n",
        "import subprocess\n",
        "\n",
        "import torch\n",
        "import pyannote.audio\n",
        "from pyannote.audio.pipelines.speaker_verification import PretrainedSpeakerEmbedding\n",
        "embedding_model = PretrainedSpeakerEmbedding(\n",
        "    \"speechbrain/spkrec-ecapa-voxceleb\",\n",
        "    device=torch.device(\"cuda\"))\n",
        "\n",
        "from pyannote.audio import Audio\n",
        "from pyannote.core import Segment\n",
        "\n",
        "import wave\n",
        "import contextlib\n",
        "\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "44qUKBbBv_R5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set Number of speakers, Language & model\n",
        "\n",
        "num_speakers = 4 #@param {type:\"integer\"}\n",
        "\n",
        "language = \"English\" #@param [\"any\", \"English\", \"Dutch\"]\n",
        "\n",
        "model_size = \"large-v2\" #@param [\"tiny\", \"base\", \"small\", \"medium\", \"large\", \"large-v2\"]\n",
        "\n",
        "\n",
        "model_name = model_size\n",
        "if language == 'English' and model_size != 'large':\n",
        "  model_name += '.en'"
      ],
      "metadata": {
        "id": "N5jDIwyjxmUM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Convert m4a file to mono wav (if needed)\n",
        "sourcePath = '/content/TypeScriptStandards-audio.m4a'\n",
        "!ffmpeg -i {sourcePath} -acodec pcm_s16le -ac 1 -ar 16000 {path}"
      ],
      "metadata": {
        "id": "7OW_B8iJcTKi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if path[-3:] != 'wav':\n",
        "  subprocess.call(['ffmpeg', '-i', path, 'audio.wav', '-y'])\n",
        "  path = 'audio.wav'\n",
        "  model = whisper.load_model(model_size)"
      ],
      "metadata": {
        "id": "0QrAK_Irxlgo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\""
      ],
      "metadata": {
        "id": "qKSdFCdiyV6G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = whisper.load_model(model_size)"
      ],
      "metadata": {
        "id": "ZLGZOLwSynPq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f69d9cdb-6bc5-47dd-f889-feb2c372f0f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████████████████████████████████| 2.87G/2.87G [00:11<00:00, 266MiB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result = model.transcribe(path)\n",
        "segments = result[\"segments\"]"
      ],
      "metadata": {
        "id": "YRl1yGbNyqUk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with contextlib.closing(wave.open(path,'r')) as f:\n",
        "  frames = f.getnframes()\n",
        "  rate = f.getframerate()\n",
        "  duration = frames / float(rate)"
      ],
      "metadata": {
        "id": "eVqU23Wqyr4t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "audio = Audio()\n",
        "\n",
        "def segment_embedding(segment):\n",
        "  start = segment[\"start\"]\n",
        "  # Whisper overshoots the end timestamp in the last segment\n",
        "  end = min(duration, segment[\"end\"])\n",
        "  clip = Segment(start, end)\n",
        "  waveform, sample_rate = audio.crop(path, clip)\n",
        "  return embedding_model(waveform[None])"
      ],
      "metadata": {
        "id": "fwnZbd2sytyJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings = np.zeros(shape=(len(segments), 192))\n",
        "for i, segment in enumerate(segments):\n",
        "  embeddings[i] = segment_embedding(segment)\n",
        "\n",
        "embeddings = np.nan_to_num(embeddings)"
      ],
      "metadata": {
        "id": "2ssdQSi9yveF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clustering = AgglomerativeClustering(num_speakers).fit(embeddings)\n",
        "labels = clustering.labels_\n",
        "for i in range(len(segments)):\n",
        "  segments[i][\"speaker\"] = 'SPEAKER ' + str(labels[i] + 1)"
      ],
      "metadata": {
        "id": "s0A2Ll80ywss"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def time(secs):\n",
        "  return datetime.timedelta(seconds=round(secs))\n",
        "\n",
        "f = open(\"transcript.txt\", \"w\", encoding=\"utf-8\")\n",
        "\n",
        "for (i, segment) in enumerate(segments):\n",
        "  if i == 0 or segments[i - 1][\"speaker\"] != segment[\"speaker\"]:\n",
        "    f.write(\"\\n\" + segment[\"speaker\"] + ' ' + str(time(segment[\"start\"])) + '\\n')\n",
        "  f.write(segment[\"text\"][1:] + ' ')\n",
        "f.close()"
      ],
      "metadata": {
        "id": "X9O4Y1PEyyJ8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}