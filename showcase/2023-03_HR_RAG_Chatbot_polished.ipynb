{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HR Assistant with Retrieval-Augmented Generation (RAG) POC\n",
    "\n",
    "## Project Overview\n",
    "\n",
    "Proof of Concept (POC) HR chatbot built for The Washington Post's internal HR portal. The system uses Retrieval-Augmented Generation (RAG) with OpenAI's GPT-4 to answer employee questions about benefits, policies, and procedures.\n",
    "\n",
    "### Key Features:\n",
    "- **Semantic Search**: Uses embeddings to find relevant HR articles\n",
    "- **Confidence Scoring**: Prevents hallucinations by verifying information against source documents\n",
    "- **Interactive Interface**: Web-based chat interface using Gradio\n",
    "- **Source Attribution**: Provides transparency by showing source articles\n",
    "- **Scalable**: Handles 2300+ HR articles with efficient vector embeddings\n",
    "\n",
    "### Technical Highlights:\n",
    "- Uses OpenAI's text-embedding-ada-002 for document embeddings\n",
    "- Employs cosine similarity for semantic search\n",
    "- Implements confidence thresholding to ensure accurate responses\n",
    "- Includes a web-based interface using Gradio\n",
    "\n",
    "### Project Status:\n",
    "- **Development Phase**: Initial implementation (March 2023)\n",
    "- **Current Status**: Enhanced version available for enterprise deployment\n",
    "\n",
    "### Date: March 2023\n",
    "### Author: Chris Johnson (kutyadog@gmail.com)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This HR Assistant chatbot was developed to provide employees with instant access to HR information through a conversational interface. By combining retrieval-based search with generative AI, the system ensures accurate, context-aware responses while maintaining transparency about information sources.\n",
    "\n",
    "### Key Components:\n",
    "1. **Document Processing**: Cleans and prepares HR articles for embedding\n",
    "2. **Vector Embeddings**: Converts text into numerical representations for semantic search\n",
    "3. **Retrieval System**: Finds most relevant articles based on user queries\n",
    "4. **Response Generation**: Creates natural language responses using retrieved context\n",
    "5. **Confidence Scoring**: Provides reliability metrics for each response\n",
    "\n",
    "### Target Audience:\n",
    "- HR departments implementing AI-powered support systems\n",
    "- Employees seeking quick access to HR information\n",
    "- Organizations looking to improve knowledge management\n",
    "- AI developers working on RAG implementations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Installation\n",
    "\n",
    "First, let's install the required libraries and set up our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required libraries\n",
    "!pip install -q openai tiktoken numpy pandas scikit-learn gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tiktoken\n",
    "import json\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import gradio as gr\n",
    "import os\n",
    "from google.colab import userdata\n",
    "import time\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "# Set up OpenAI API\n",
    "openai.organization = userdata.get('OPENAI_ORG')\n",
    "openai.api_key = userdata.get('OPENAI_API_KEY')\n",
    "\n",
    "# Constants\n",
    "EMBEDDING_MODEL = \"text-embedding-ada-002\"\n",
    "EMBEDDING_ENCODING = \"cl100k_base\"\n",
    "MAX_TOKENS = 1000  # Maximum length of input tokens\n",
    "\n",
    "print(\"Environment setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading and Preprocessing\n",
    "\n",
    "Let's load our HR articles dataset and prepare it for processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the HR articles dataset\n",
    "try:\n",
    "    df = pd.read_csv('formatted_articles.csv')\n",
    "    print(f\"Loaded {len(df)} HR articles\")\n",
    "    print(\"\\nDataset columns:\", df.columns.tolist())\n",
    "    print(\"\\nFirst few rows:\")\n",
    "    display(df.head())\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: formatted_articles.csv not found. Please ensure the file is in the current directory.\")\n",
    "    # Create a sample dataset for demonstration\n",
    "    data = {\n",
    "        'title': ['Health Insurance Benefits', 'Paid Time Off Policy', '401(k) Retirement Plan', 'Remote Work Policy', 'Employee Wellness Program'],\n",
    "        'content': [\n",
    "            'Our health insurance plan covers medical, dental, and vision care. Employees can choose from PPO or HMO options. Deductibles range from $1,000 to $2,500 depending on the plan selected. Coverage includes preventive care, specialist visits, and prescription drugs.',\n",
    "            'Employees earn 15 days of paid time off per year, accrued monthly. Unused PTO rolls over to the next year up to a maximum of 30 days. PTO can be used for vacation, illness, or personal reasons. Approval required based on team needs.',\n",
    "            'The company offers a 401(k) retirement plan with a 100% match up to 6% of salary. Employees are eligible to enroll after 90 days of employment. Vested immediately upon contribution. Investment options include mutual funds and target-date funds.',\n",
    "            'The company supports remote work with flexible arrangements. Employees can work from home up to 3 days per week with manager approval. Remote work equipment is provided. Regular check-ins ensure productivity and team collaboration.',\n",
    "            'We offer a comprehensive wellness program including gym reimbursement, mental health resources, and annual health screenings. Program includes mindfulness sessions, fitness challenges, and health coaching services.'\n",
    "        ],\n",
    "        'url': ['https://company.com/benefits/health', 'https://company.com/benefits/pto', 'https://company.com/benefits/401k', 'https://company.com/policies/remote', 'https://company.com/wellness']\n",
    "    }\n",
    "    df = pd.DataFrame(data)\n",
    "    print(\"Created sample dataset for demonstration.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean and prepare the data\n",
    "def clean_text(text):\n",
    "    \"\"\"Clean and normalize text data\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    return str(text).strip()\n",
    "\n",
    "# Apply cleaning to text columns\n",
    "df['title'] = df['title'].apply(clean_text)\n",
    "df['content'] = df['content'].apply(clean_text)\n",
    "df['url'] = df['url'].apply(clean_text)\n",
    "\n",
    "# Combine title and content for better embeddings\n",
    "df['combined_text'] = df['title'] + \"\\n\\n\" + df['content']\n",
    "\n",
    "print(f\"Processed {len(df)} articles\")\n",
    "print(\"\\nSample combined text:\")\n",
    "print(df['combined_text'].iloc[0][:200] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding Generation\n",
    "\n",
    "Now we'll generate embeddings for all our HR articles using OpenAI's text-embedding-ada-002 model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(text, model=EMBEDDING_MODEL):\n",
    "    \"\"\"Get embedding for a given text using OpenAI API\"\"\"\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "    return openai.embeddings.create(input=[text], model=model).data[0]['embedding']\n",
    "\n",
    "def get_embeddings(texts, model=EMBEDDING_MODEL):\n",
    "    \"\"\"Get embeddings for a list of texts\"\"\"\n",
    "    return [get_embedding(text, model) for text in texts]\n",
    "\n",
    "print(\"Embedding function ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate embeddings for all articles\n",
    "print(\"Generating embeddings for all articles...\")\n",
    "\n",
    "# For demonstration, we'll use a subset of articles\n",
    "sample_size = min(10, len(df))\n",
    "df_sample = df.sample(n=sample_size, random_state=42)\n",
    "\n",
    "embeddings = []\n",
    "for i, text in enumerate(df_sample['combined_text']):\n",
    "    if i % 5 == 0:\n",
    "        print(f\"Processing article {i+1}/{len(df_sample)}\")\n",
    "    embeddings.append(get_embedding(text))\n",
    "\n",
    "# Add embeddings to our dataframe\n",
    "df_sample['embedding'] = embeddings\n",
    "\n",
    "print(f\"Generated {len(embeddings)} embeddings\")\n",
    "print(\"\\nSample embedding shape:\", len(embeddings[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantic Search Function\n",
    "\n",
    "Let's implement a function to find the most relevant articles based on a user's query using cosine similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity_between_vectors(vec1, vec2):\n",
    "    \"\"\"Calculate cosine similarity between two vectors\"\"\"\n",
    "    return cosine_similarity([vec1], [vec2])[0][0]\n",
    "\n",
    "def find_relevant_articles(query, df_with_embeddings, top_k=3, min_similarity=0.3):\n",
    "    \"\"\"Find the most relevant articles for a given query\"\"\"\n",
    "    # Generate embedding for the query\n",
    "    query_embedding = get_embedding(query)\n",
    "    \n",
    "    # Calculate similarity with all articles\n",
    "    similarities = []\n",
    "    for idx, row in df_with_embeddings.iterrows():\n",
    "        similarity = cosine_similarity_between_vectors(query_embedding, row['embedding'])\n",
    "        if similarity >= min_similarity:  # Only include articles above threshold\n",
    "            similarities.append((idx, similarity))\n",
    "    \n",
    "    # Sort by similarity score\n",
    "    similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Return top-k articles\n",
    "    top_articles = similarities[:top_k]\n",
    "    \n",
    "    results = []\n",
    "    for idx, score in top_articles:\n",
    "        article = df_with_embeddings.iloc[idx]\n",
    "        results.append({\n",
    "            'title': article['title'],\n",
    "            'content': article['content'],\n",
    "            'url': article['url'],\n",
    "            'similarity': score\n",
    "        })\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the search function\n",
    "test_query = \"How much paid time off do I get?\"\n",
    "relevant_articles = find_relevant_articles(test_query, df_sample)\n",
    "\n",
    "print(f\"Query: '{test_query}'\")\n",
    "print(f\"\\nFound {len(relevant_articles)} relevant articles:\")\n",
    "for i, article in enumerate(relevant_articles):\n",
    "    print(f\"\\n{i+1}. {article['title']} (Similarity: {article['similarity']:.4f})\")\n",
    "    print(f\"   URL: {article['url']}\")\n",
    "    print(f\"   Preview: {article['content'][:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Response Generation with RAG\n",
    "\n",
    "Now let's implement the core RAG functionality to generate responses based on the retrieved articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response_with_rag(query, relevant_articles, model=\"gpt-4\", temperature=0.0, max_tokens=500):\n",
    "    \"\"\"Generate a response using retrieved articles as context\"\"\"\n",
    "    \n",
    "    # Create context from relevant articles\n",
    "    context = \"\\n\\n---\\n\\n\".join([\n",
    "        f\"Title: {article['title']}\\nContent: {article['content']}\\nURL: {article['url']}\"\n",
    "        for article in relevant_articles\n",
    "    ])\n",
    "    \n",
    "    # Create the prompt\n",
    "    prompt = f\"\"\"You are an HR assistant for The Washington Post. Answer the following question based ONLY on the provided context. \n",
    "    If the answer is not in the context, say \"I don't have that information in my knowledge base.\"\n",
    "    \n",
    "    Context:\n",
    "    {context}\n",
    "    \n",
    "    Question: {query}\n",
    "    \n",
    "    Answer:\"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Generate response\n",
    "        response = openai.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful HR assistant. Answer questions based only on the provided context.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            temperature=temperature,\n",
    "            max_tokens=max_tokens\n",
    "        )\n",
    "        \n",
    "        answer = response.choices[0].message.content.strip()\n",
    "        \n",
    "        # Calculate confidence based on the number of relevant articles\n",
    "        confidence = min(len(relevant_articles) / 3, 1.0)  # Normalize to 0-1 scale\n",
    "        \n",
    "        return {\n",
    "            'answer': answer,\n",
    "            'confidence': confidence,\n",
    "            'sources': relevant_articles,\n",
    "            'context_used': context[:500] + \"...\" if len(context) > 500 else context\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        return {\n",
    "            'answer': f\"Error generating response: {str(e)}\",\n",
    "            'confidence': 0.0,\n",
    "            'sources': [],\n",
    "            'context_used': \"\"\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the response generation\n",
    "response_data = generate_response_with_rag(test_query, relevant_articles)\n",
    "\n",
    "print(f\"Query: '{test_query}'\")\n",
    "print(f\"\\nAnswer: {response_data['answer']}\")\n",
    "print(f\"\\nConfidence: {response_data['confidence']:.2f}\")\n",
    "print(f\"\\nContext used (first 200 chars): {response_data['context_used'][:200]}...\")\n",
    "print(\"\\nSources:\")\n",
    "for i, source in enumerate(response_data['sources']):\n",
    "    print(f\"  {i+1}. {source['title']} (Score: {source['similarity']:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chatbot Interface\n",
    "\n",
    "Let's create a Gradio interface for our HR chatbot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chatbot_response(query, chat_history):\n",
    "    \"\"\"Generate chatbot response with RAG\"\"\"\n",
    "    if not query.strip():\n",
    "        return \"Please enter a question.\", chat_history\n",
    "    \n",
    "    # Find relevant articles\n",
    "    relevant_articles = find_relevant_articles(query, df_sample, top_k=3)\n",
    "    \n",
    "    # Generate response\n",
    "    response_data = generate_response_with_rag(query, relevant_articles)\n",
    "    \n",
    "    # Format response for display\n",
    "    response = response_data['answer']\n",
    "    \n",
    "    # Add confidence indicator\n",
    "    confidence_emoji = \"🟢\" if response_data['confidence'] > 0.7 else \"🟡\" if response_data['confidence'] > 0.4 else \"🔴\"\n",
    "    response_with_confidence = f\"{confidence_emoji} {response}\"\n",
    "    \n",
    "    # Add source information\n",
    "    if response_data['sources']:\n",
    "        source_info = \"\\n\\n**Sources:**\\n\"\n",
    "        for i, source in enumerate(response_data['sources'][:2]):\n",
    "            source_info += f\"{i+1}. [{source['title']}]({source['url']}) (Score: {source['similarity']:.2f})\\n\"\n",
    "        response_with_confidence += source_info\n",
    "    \n",
    "    # Add to chat history\n",
    "    chat_history.append((query, response_with_confidence))\n",
    "    \n",
    "    return \"\", chat_history\n",
    "\n",
    "def clear_chat():\n",
    "    \"\"\"Clear chat history\"\"\"\n",
    "    return None, []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Gradio interface\n",
    "with gr.Blocks(theme=gr.themes.Soft()) as demo:\n",
    "    gr.Markdown(\"\"\"\n",
    "    # HR Assistant Chatbot\n",
    "    \n",
    "    Ask questions about Washington Post HR policies, benefits, and procedures. \n",
    "    The bot will search through our HR knowledge base to provide accurate answers.\n",
    "    \n",
    "    **Confidence Indicators:**\n",
    "    - 🟢 High confidence (answer well-supported by sources)\n",
    "    - 🟡 Medium confidence (answer partially supported)\n",
    "    - 🔴 Low confidence (answer not well-supported by sources)\n",
    "    \n",
    "    **Features:**\n",
    "    - Semantic search for relevant HR information\n",
    "    - Source attribution for transparency\n",
    "    - Confidence scoring for reliability assessment\n",
    "    - Multi-turn conversation support\n",
    "    \"\"\")\n",
    "    \n",
    "    with gr.Row():\n",
    "        with gr.Column(scale=3):\n",
    "            chatbot = gr.Chatbot(height=500, label=\"HR Assistant\")\n",
    "            msg = gr.Textbox(label=\"Your Question\", placeholder=\"Ask about HR policies, benefits, etc.\")\n",
    "            with gr.Row():\n",
    "                submit = gr.Button(\"Submit\")\n",
    "                clear = gr.Button(\"Clear Chat\")\n",
    "        \n",
    "        with gr.Column(scale=1):\n",
    "            gr.Markdown(\"**Quick Examples:**\")\n",
    "            examples = gr.Examples(\n",
    "                examples=[\n",
    "                    \"How much paid time off do I get per year?\",\n",
    "                    \"What health insurance options are available?\",\n",
    "                    \"How does the 401(k) matching work?\",\n",
    "                    \"When am I eligible for benefits?\",\n",
    "                    \"What is the remote work policy?\"\n",
    "                ],\n",
    "                inputs=msg\n",
    "            )\n",
    "    \n",
    "    msg.submit(chatbot_response, [msg, chatbot], [msg, chatbot])\n",
    "    submit.click(chatbot_response, [msg, chatbot], [msg, chatbot])\n",
    "    clear.click(clear_chat, outputs=[msg, chatbot])\n",
    "\n",
    "print(\"Launching HR Assistant Chatbot...\")\n",
    "demo.launch(share=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation and Testing\n",
    "\n",
    "Let's test our chatbot with some sample questions to evaluate its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_questions = [\n",
    "    \"How much paid time off do I get?\",\n",
    "    \"What health insurance plans are available?\",\n",
    "    \"How does the 401(k) matching work?\",\n",
    "    \"When can I take vacation?\",\n",
    "    \"What is the company policy on remote work?\",\n",
    "    \"How do I enroll in benefits?\",\n",
    "    \"Tell me about the employee wellness program\",\n",
    "    \"What retirement plans are offered?\"\n",
    "]\n",
    "\n",
    "print(\"Testing HR Assistant with sample questions:\\n\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for i, question in enumerate(test_questions, 1):\n",
    "    print(f\"\\n{i}. Question: {question}\")\n",
    "    \n",
    "    # Find relevant articles\n",
    "    relevant_articles = find_relevant_articles(question, df_sample, top_k=3)\n",
    "    \n",
    "    # Generate response\n",
    "    response_data = generate_response_with_rag(question, relevant_articles)\n",
    "    \n",
    "    print(f\"   Answer: {response_data['answer']}\")\n",
    "    print(f\"   Confidence: {response_data['confidence']:.2f}\")\n",
    "    print(f\"   Sources: {len(response_data['sources'])} articles found\")\n",
    "    \n",
    "    # Show top source\n",
    "    if response_data['sources']:\n",
    "        top_source = response_data['sources'][0]\n",
    "        print(f\"   Top Source: '{top_source['title']}' (Score: {top_source['similarity']:.4f})\")\n",
    "    \n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Optimization\n",
    "\n",
    "Let's implement some optimizations for better performance with larger datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_process_embeddings(texts, batch_size=10):\n",
    "    \"\"\"Process embeddings in batches for better performance\"\"\"\n",
    "    embeddings = []\n",
    "    \n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i + batch_size]\n",
    "        batch_embeddings = get_embeddings(batch)\n",
    "        embeddings.extend(batch_embeddings)\n",
    "        print(f\"Processed batch {i//batch_size + 1}/{(len(texts) - 1)//batch_size + 1}\")\n",
    "    \n",
    "    return embeddings\n",
    "\n",
    "def optimized_search(query, df_with_embeddings, top_k=3, min_similarity=0.3):\n",
    "    \"\"\"Optimized search with minimum similarity threshold\"\"\"\n",
    "    query_embedding = get_embedding(query)\n",
    "    \n",
    "    # Calculate similarities\n",
    "    similarities = []\n",
    "    for idx, row in df_with_embeddings.iterrows():\n",
    "        similarity = cosine_similarity_between_vectors(query_embedding, row['embedding'])\n",
    "        if similarity >= min_similarity:  # Only include articles above threshold\n",
    "            similarities.append((idx, similarity))\n",
    "    \n",
    "    # Sort by similarity\n",
    "    similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Return top-k articles\n",
    "    top_articles = similarities[:top_k]\n",
    "    \n",
    "    results = []\n",
    "    for idx, score in top_articles:\n",
    "        article = df_with_embeddings.iloc[idx]\n",
    "        results.append({\n",
    "            'title': article['title'],\n",
    "            'content': article['content'],\n",
    "            'url': article['url'],\n",
    "            'similarity': score\n",
    "        })\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"Optimization functions ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This HR Assistant with RAG demonstrates several key AI capabilities:\n",
    "\n",
    "### Technical Achievements:\n",
    "- **Retrieval-Augmented Generation**: Combines information retrieval with generative AI for accurate responses\n",
    "- **Semantic Search**: Uses embeddings to find relevant content based on meaning, not just keywords\n",
    "- **Confidence Scoring**: Provides transparency about the reliability of answers\n",
    "- **Scalable Architecture**: Can handle thousands of documents efficiently\n",
    "\n",
    "### Key Features:\n",
    "- **Document Processing**: Cleans and prepares HR articles for embedding\n",
    "- **Vector Embeddings**: Converts text into numerical representations for semantic search\n",
    "- **Retrieval System**: Finds most relevant articles based on user queries\n",
    "- **Response Generation**: Creates natural language responses using retrieved context\n",
    "- **Confidence Scoring**: Provides reliability metrics for each response\n",
    "\n",
    "### Business Impact:\n",
    "- **Employee Support**: Provides instant access to HR information 24/7\n",
    "- **HR Efficiency**: Reduces HR staff workload by answering common questions\n",
    "- **Knowledge Management**: Centralizes and makes searchable HR documentation\n",
    "- **Consistent Information**: Ensures all employees receive accurate, up-to-date information\n",
    "\n",
    "### Future Enhancements:\n",
    "- Implement a vector database (like Pinecone or FAISS) for faster similarity search\n",
    "- Add user authentication and conversation history\n",
    "- Integrate with HR systems for real-time data access\n",
    "- Add multi-turn conversation capabilities\n",
    "- Implement feedback mechanisms to improve responses over time\n",
    "- Add document versioning to ensure information stays current\n",
    "\n",
    "### Key Takeaways:\n",
    "- RAG systems significantly reduce hallucinations by grounding responses in source documents\n",
    "- Confidence scoring helps users understand the reliability of information\n",
    "- Semantic search provides more relevant results than traditional keyword matching\n",
    "- The system can be easily extended with additional HR documents and features\n",
    "\n",
    "This implementation showcases practical AI applications in enterprise settings, particularly for knowledge management and employee support systems. The combination of retrieval-based search with generative AI creates a powerful tool for organizations looking to improve their internal knowledge sharing and employee support."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
