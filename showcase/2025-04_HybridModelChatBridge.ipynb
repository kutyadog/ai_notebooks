{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hybrid Model Architecture POC - Chat with Base model via Instruct model Bridge\n",
    "\n",
    "## Project Overview\n",
    "\n",
    "This Colab notebook provides a proof-of-concept for integrating an AI chatbot to bridge user communication with a local cybersecurity base transformer model (fdtn-ai/Foundation-Sec-8B).\n",
    "\n",
    "In April 2025, Cisco released Foundation-Sec-8B, a specialized AI base-model for cybersecurity scenarios. The system provides expert-level insights on vulnerabilities, exploits, and security best practices. I wanted to chat with it, but BASE models are NOT ideal for direct conversational use. It's primary function is to predict the next most likely word in a sequence. If you ask it a question like \"What is the capital of France?\", it might complete the prompt with something like \"What is the largest city in France?\". It doesn't inherently understand the concept of a question-and-answer format.\n",
    "\n",
    "### Task Summary:\n",
    "I wanted to see if I could bridge that limitation by placing a chat/instruct model in front of the base model, so that it could help communicate (reformat the questions) with the base model.\n",
    "\n",
    "### Key Goals:\n",
    "- **Utilize BASE Model in Chat**: Use Foundation-Sec-8B in chat.\n",
    "- **Utilize Chat/Instruct Model as 'Interpreter'**: Use chat/instruct model as 'interpreter' (reformat the questions) between user and base model.\n",
    "- **Multi-Model Architecture**: Combines specialized and general-purpose models\n",
    "- **Interactive Chat Interface**: User-friendly Gradio-based interface\n",
    "- **Performance Optimization**: Efficient model loading and response generation\n",
    "\n",
    "### Project Status:\n",
    "- **Development Phase**: Initial implementation (April 2025)\n",
    "- **Testing Phase**: Comprehensive evaluation\n",
    "- **Current Status**: Done.\n",
    "- \n",
    "### Key Takeaways:\n",
    "- Not great results! For tests I used **Llama-3.1-8B-Instruct** as interpreter.\n",
    "- Though not unexpected, it's not just plug-and-play. An instruct model can not intuitively bridge the gap between Base model and user. Though, I do believe with more prompt contextual training and/or fine-tuning, and/or even using a different instruct/chat model, we might have better success.\n",
    "- Confidence scoring helps users assess the reliability of information\n",
    "- Performance metrics are essential for evaluating AI system effectiveness\n",
    "\n",
    "### Date: April 2025\n",
    "- **Development Phase**: Initial implementation (April 2025)\n",
    "\n",
    "### Update August 2025\n",
    "- **New Release**: Cisco released an Instruct version \n",
    "https://huggingface.co/fdtn-ai/Foundation-Sec-8B-Instruct\n",
    "Test still relevant for other base models.\n",
    "Alternate solution: Learn how to fine-tune a base model myself to convert to instruct version.\n",
    "\n",
    "I built a basic next.js test for running:\n",
    "https://hundley-sec-ai.vercel.app/\n",
    "\n",
    "### Author: Chris Johnson (kutyadog@gmail.com)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Installation\n",
    "\n",
    "First, let's install the required libraries and set up our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required libraries\n",
    "!pip install -q torch transformers gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import gradio as gr\n",
    "import os\n",
    "import time\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "# Set device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Model options\n",
    "BASE_MODEL = \"fdtn-ai/Foundation-Sec-8B\"\n",
    "INSTRUCT_MODEL = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "\n",
    "print(\"Environment setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Loading\n",
    "\n",
    "Let's load the cybersecurity model and tokenizer. We'll provide options for both the base and instruct versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_name):\n",
    "    \"\"\"Load the specified cybersecurity model\"\"\"\n",
    "    try:\n",
    "        print(f\"Loading model: {model_name}\")\n",
    "        \n",
    "        # Load tokenizer\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        \n",
    "        # Set pad token if not present\n",
    "        if tokenizer.pad_token is None:\n",
    "            tokenizer.pad_token = tokenizer.eos_token\n",
    "        \n",
    "        # Load model with appropriate settings\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            torch_dtype=torch.float16 if device == \"cuda\" else torch.float32,\n",
    "            device_map=\"auto\" if device == \"cuda\" else None,\n",
    "            low_cpu_mem_usage=True\n",
    "        )\n",
    "        \n",
    "        print(\"Model loaded successfully!\")\n",
    "        return tokenizer, model\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model: {e}\")\n",
    "        return None, None\n",
    "\n",
    "# Load models\n",
    "print(\"Loading cybersecurity models...\")\n",
    "base_tokenizer, base_model = load_model(BASE_MODEL)\n",
    "instruct_tokenizer, instruct_model = load_model(INSTRUCT_MODEL)\n",
    "\n",
    "# Set current model to instruct model if available, otherwise use base model\n",
    "current_model = instruct_model if instruct_model else base_model\n",
    "current_tokenizer = instruct_tokenizer if instruct_tokenizer else base_tokenizer\n",
    "\n",
    "if current_model is None:\n",
    "    print(\"Error: Could not load any model. Please check your internet connection and try again.\")\n",
    "else:\n",
    "    print(f\"Current model: {INSTRUCT_MODEL if instruct_model else BASE_MODEL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Core Chatbot Class\n",
    "\n",
    "Let's implement an optimized chatbot class for cybersecurity assistance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SecurityChatbot:\n",
    "    \"\"\"Optimized cybersecurity chatbot class\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str = INSTRUCT_MODEL):\n",
    "        self.model_name = model_name\n",
    "        self.tokenizer = None\n",
    "        self.model = None\n",
    "        self.device = device\n",
    "        self.load_time = 0\n",
    "        self.is_loaded = False\n",
    "        \n",
    "    def load_model(self) -> bool:\n",
    "        \"\"\"Load the model with error handling\"\"\"\n",
    "        try:\n",
    "            start_time = time.time()\n",
    "            print(f\"Loading {self.model_name}...\")\n",
    "            \n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
    "            \n",
    "            # Set pad token if not present\n",
    "            if self.tokenizer.pad_token is None:\n",
    "                self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "            \n",
    "            self.model = AutoModelForCausalLM.from_pretrained(\n",
    "                self.model_name,\n",
    "                torch_dtype=torch.float16 if self.device == \"cuda\" else torch.float32,\n",
    "                device_map=\"auto\" if self.device == \"cuda\" else None,\n",
    "                low_cpu_mem_usage=True\n",
    "            )\n",
    "            \n",
    "            self.load_time = time.time() - start_time\n",
    "            self.is_loaded = True\n",
    "            print(f\"Model loaded in {self.load_time:.2f} seconds\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Failed to load model: {e}\")\n",
    "            self.is_loaded = False\n",
    "            return False\n",
    "    \n",
    "    def generate_response(\n",
    "        self, \n",
    "        prompt: str, \n",
    "        max_new_tokens: int = 150,\n",
    "        temperature: float = 0.1,\n",
    "        top_p: float = 0.9\n",
    "    ) -> dict:\n",
    "        \"\"\"Generate response with optimized parameters\"\"\"\n",
    "        if not self.is_loaded:\n",
    "            return {\n",
    "                'response': \"Model not loaded. Please wait for model initialization.\",\n",
    "                'confidence': 0.0,\n",
    "                'sources': []\n",
    "            }\n",
    "        \n",
    "        try:\n",
    "            # Preprocess prompt\n",
    "            if not prompt.strip():\n",
    "                return {\n",
    "                    'response': \"Please enter a valid security question.\",\n",
    "                    'confidence': 0.0,\n",
    "                    'sources': []\n",
    "                }\n",
    "            \n",
    "            # Format prompt for cybersecurity context\n",
    "            formatted_prompt = self._format_security_prompt(prompt)\n",
    "            \n",
    "            # Tokenize with attention mask\n",
    "            inputs = self.tokenizer(\n",
    "                formatted_prompt, \n",
    "                return_tensors=\"pt\", \n",
    "                padding=True, \n",
    "                truncation=True,\n",
    "                max_length=512\n",
    "            ).to(self.device)\n",
    "            \n",
    "            # Generate response\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model.generate(\n",
    "                    inputs.input_ids,\n",
    "                    attention_mask=inputs.attention_mask,\n",
    "                    max_new_tokens=max_new_tokens,\n",
    "                    do_sample=True,\n",
    "                    temperature=temperature,\n",
    "                    top_p=top_p,\n",
    "                    pad_token_id=self.tokenizer.pad_token_id,\n",
    "                    eos_token_id=self.tokenizer.eos_token_id,\n",
    "                    repetition_penalty=1.1\n",
    "                )\n",
    "            \n",
    "            # Decode and clean response\n",
    "            response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "            response = response.replace(formatted_prompt, \"\").strip()\n",
    "            \n",
    "            # Post-process response\n",
    "            response = self._post_process_response(response)\n",
    "            \n",
    "            # Calculate confidence based on response quality\n",
    "            confidence = self._calculate_confidence(response, prompt)\n",
    "            \n",
    "            return {\n",
    "                'response': response if response else \"I couldn't generate a meaningful response. Please try rephrasing your question.\",\n",
    "                'confidence': confidence,\n",
    "                'sources': [self.model_name]\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            return {\n",
    "                'response': f\"Error: {str(e)}\",\n",
    "                'confidence': 0.0,\n",
    "                'sources': []\n",
    "            }\n",
    "    \n",
    "    def _format_security_prompt(self, query: str) -> str:\n",
    "        \"\"\"Format the prompt for cybersecurity context\"\"\"\n",
    "        return f\"\"\"As an expert cybersecurity analyst, provide a detailed analysis of the following query:\n",
    "\n",
    "Query: {query}\n",
    "\n",
    "Please provide:\n",
    "1. Technical explanation of the security concept/vulnerability\n",
    "2. Potential impacts and risks\n",
    "3. Mitigation strategies or best practices\n",
    "4. Related information if applicable\n",
    "\n",
    "Expert Analysis:\"\"\"\n",
    "    \n",
    "    def _post_process_response(self, response: str) -> str:\n",
    "        \"\"\"Clean up the generated response\"\"\"\n",
    "        # Remove extra whitespace\n",
    "        response = \" \".join(response.split())\n",
    "        \n",
    "        # Remove incomplete sentences at the end\n",
    "        sentences = response.split('. ')\n",
    "        if len(sentences) > 1 and len(sentences[-1]) < 20:\n",
    "            response = '. '.join(sentences[:-1]) + '.'\n",
    "        \n",
    "        return response\n",
    "    \n",
    "    def _calculate_confidence(self, response: str, query: str) -> float:\n",
    "        \"\"\"Calculate confidence score for the response\"\"\"\n",
    "        if not response or \"Error\" in response:\n",
    "            return 0.0\n",
    "        \n",
    "        # Simple heuristics for confidence scoring\n",
    "        confidence = 0.5  # Base confidence\n",
    "        \n",
    "        # Increase confidence for longer, more detailed responses\n",
    "        if len(response) > 100:\n",
    "            confidence += 0.2\n",
    "        if len(response) > 200:\n",
    "            confidence += 0.1\n",
    "        \n",
    "        # Check for security-related keywords\n",
    "        security_keywords = ['vulnerability', 'exploit', 'mitigation', 'protection', 'attack', 'defense', 'risk', 'threat']\n",
    "        keyword_count = sum(1 for keyword in security_keywords if keyword.lower() in response.lower())\n",
    "        confidence += min(keyword_count * 0.05, 0.2)\n",
    "        \n",
    "        return min(confidence, 1.0)\n",
    "    \n",
    "    def get_model_info(self) -> dict:\n",
    "        \"\"\"Get information about the loaded model\"\"\"\n",
    "        return {\n",
    "            \"model_name\": self.model_name,\n",
    "            \"device\": self.device,\n",
    "            \"load_time\": self.load_time,\n",
    "            \"loaded\": self.is_loaded\n",
    "        }\n",
    "\n",
    "# Initialize the optimized chatbot\n",
    "security_bot = SecurityChatbot()\n",
    "if security_bot.load_model():\n",
    "    model_info = security_bot.get_model_info()\n",
    "    print(f\"Model info: {model_info}\")\n",
    "else:\n",
    "    print(\"Failed to initialize security chatbot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Vulnerability Analysis\n",
    "\n",
    "Let's test the model with some cybersecurity examples to demonstrate its capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response_old(prompt, model, tokenizer, max_new_tokens=100, temperature=0.1):\n",
    "    \"\"\"Legacy response generation function\"\"\"\n",
    "    if model is None or tokenizer is None:\n",
    "        return \"Model not loaded.\"\n",
    "    \n",
    "    try:\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                inputs.input_ids,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                do_sample=True,\n",
    "                temperature=temperature,\n",
    "                top_p=0.9,\n",
    "                pad_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "        \n",
    "        response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        # Remove the original prompt from the response\n",
    "        response = response.replace(prompt, \"\").strip()\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        return f\"Error generating response: {str(e)}\"\n",
    "\n",
    "# Example prompt for CWE to CVE matching\n",
    "cwe_prompt = \"\"\"CVE-2021-44228 is a remote code execution flaw in Apache Log4j2 via unsafe JNDI lookups (\"Log4Shell\"). The CWE is CWE-502.\n",
    "\n",
    "CVE-2017-0144 is a remote code execution vulnerability in Microsoft's SMBv1 server (\"EternalBlue\") due to a buffer overflow. The CWE is CWE-119.\n",
    "\n",
    "CVE-2014-0160 is an information-disclosure bug in OpenSSL's heartbeat extension (\"Heartbleed\") causing out-of-bounds reads. The CWE is CWE-125.\n",
    "\n",
    "CVE-2017-5638 is a remote code execution issue in Apache Struts 2's Jakarta Multipart parser stemming from improper input validation of the Content-Type header. The CWE is CWE-20.\n",
    "\n",
    "CVE-2019-0708 is a remote code execution vulnerability in Microsoft's Remote Desktop Services (\"BlueKeep\") triggered by a use-after-free. The CWE is CWE-416.\n",
    "\n",
    "CVE-2015-10011 is a vulnerability about OpenDNS OpenResolve improper log output neutralization. The CWE is\"\"\"\n",
    "\n",
    "print(\"Testing cybersecurity model with CWE to CVE matching...\")\n",
    "response = generate_response_old(cwe_prompt, current_model, current_tokenizer)\n",
    "print(f\"\\nResponse: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interactive Chat Interface\n",
    "\n",
    "Let's create a Gradio interface for our cybersecurity chatbot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(message, chat_history):\n",
    "    \"\"\"Generate a response to the user's message\"\"\"\n",
    "    print(f\"User message: {message}\")\n",
    "    \n",
    "    # Generate response using the optimized chatbot\n",
    "    result = security_bot.generate_response(message, max_new_tokens=150, temperature=0.2)\n",
    "    \n",
    "    response = result['response']\n",
    "    confidence = result['confidence']\n",
    "    \n",
    "    # Add confidence indicator\n",
    "    confidence_emoji = \"🟢\" if confidence > 0.7 else \"🟡\" if confidence > 0.4 else \"🔴\"\n",
    "    response_with_confidence = f\"{confidence_emoji} {response}\"\n",
    "    \n",
    "    print(f\"Model response: {response}\")\n",
    "    print(f\"Confidence: {confidence:.2f}\")\n",
    "    \n",
    "    # Add to chat history\n",
    "    chat_history.append((message, response_with_confidence))\n",
    "    \n",
    "    return \"\", chat_history\n",
    "\n",
    "def clear_chat():\n",
    "    \"\"\"Clear the chat history\"\"\"\n",
    "    return None, []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Gradio interface\n",
    "with gr.Blocks(theme=gr.themes.Soft()) as demo:\n",
    "    gr.Markdown(\"\"\"\n",
    "    # Cybersecurity AI Assistant\n",
    "    \n",
    "    This chatbot uses Cisco's Foundation-Sec-8B model to provide expert-level cybersecurity insights.\n",
    "    Ask questions about vulnerabilities, exploits, security best practices, and more.\n",
    "    \n",
    "    **Model**: Foundation-Sec-8B-Instruct\n",
    "    **Specialization**: Cybersecurity vulnerabilities, exploits, and defense strategies\n",
    "    \n",
    "    **Confidence Indicators:**\n",
    "    - 🟢 High confidence (detailed, accurate response)\n",
    "    - 🟡 Medium confidence (partial information)\n",
    "    - 🔴 Low confidence (limited or uncertain response)\n",
    "    \"\"\")\n",
    "    \n",
    "    with gr.Row():\n",
    "        with gr.Column(scale=3):\n",
    "            chatbot = gr.Chatbot(height=500, label=\"Security Assistant\")\n",
    "            msg = gr.Textbox(label=\"Your Security Question\", placeholder=\"Ask about vulnerabilities, exploits, security best practices, etc.\")\n",
    "            with gr.Row():\n",
    "                submit = gr.Button(\"Submit\")\n",
    "                clear = gr.Button(\"Clear Chat\")\n",
    "        \n",
    "        with gr.Column(scale=1):\n",
    "            gr.Markdown(\"**Quick Examples:**\")\n",
    "            examples = gr.Examples(\n",
    "                examples=[\n",
    "                    \"What is CVE eternal blue and how does it work?\",\n",
    "                    \"Explain the difference between CWE-119 and CWE-20\",\n",
    "                    \"What are the best practices for preventing Log4Shell attacks?\",\n",
    "                    \"How does a buffer overflow vulnerability work?\",\n",
    "                    \"What is MITRE ATT&CK framework and how is it used?\"\n",
    "                ],\n",
    "                inputs=msg\n",
    "            )\n",
    "    \n",
    "    msg.submit(predict, [msg, chatbot], [msg, chatbot])\n",
    "    submit.click(predict, [msg, chatbot], [msg, chatbot])\n",
    "    clear.click(clear_chat, outputs=[msg, chatbot])\n",
    "\n",
    "print(\"Launching Cybersecurity AI Assistant...\")\n",
    "demo.launch(share=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternative Implementation with Gradio Interface\n",
    "\n",
    "Let's also implement a simpler version using Gradio's Interface component for direct text interaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_simple(message):\n",
    "    \"\"\"Simple prediction function for direct text interaction\"\"\"\n",
    "    print(f\"Processing message: {message}\")\n",
    "    \n",
    "    if not message.strip():\n",
    "        return \"Please enter a security question.\"\n",
    "    \n",
    "    # Generate response using the optimized chatbot\n",
    "    result = security_bot.generate_response(message, max_new_tokens=200, temperature=0.1)\n",
    "    \n",
    "    response = result['response']\n",
    "    confidence = result['confidence']\n",
    "    \n",
    "    # Add confidence indicator\n",
    "    confidence_emoji = \"🟢\" if confidence > 0.7 else \"🟡\" if confidence > 0.4 else \"🔴\"\n",
    "    response_with_confidence = f\"{confidence_emoji} {response}\"\n",
    "    \n",
    "    return response_with_confidence\n",
    "\n",
    "# Create a simpler interface\n",
    "iface = gr.Interface(\n",
    "    fn=predict_simple,\n",
    "    inputs=[\"text\"],\n",
    "    outputs=[\"text\"],\n",
    "    title=\"Foundation-Sec-8B Security Assistant\",\n",
    "    description=\"Ask cybersecurity questions and get expert insights from Cisco's Foundation-Sec-8B model.\n",
    "    \n",
    "    **Features:**\n",
    "    - Specialized cybersecurity knowledge\n",
    "    - Detailed vulnerability analysis\n",
    "    - Security best practices\n",
    "    - Confidence scoring for reliability\",\n",
    "    examples=[\n",
    "        [\"What is CVE-2021-44228 (Log4Shell) and how can I protect my systems?\"],\n",
    "        [\"Explain the difference between authentication and authorization in cybersecurity\"],\n",
    "        [\"What are the OWASP Top 10 vulnerabilities?\"],\n",
    "        [\"How does a zero-day vulnerability work?\"],\n",
    "        [\"What is the MITRE ATT&CK framework?\"]\n",
    "    ],\n",
    "    article=\"<p>This tool uses Cisco's Foundation-Sec-8B model, a specialized cybersecurity model trained on security-related data.</p>\"\n",
    ")\n",
    "\n",
    "print(\"Launching simple security interface...\")\n",
    "iface.launch(share=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Comparison\n",
    "\n",
    "Let's compare the responses from both the base and instruct models to see which performs better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_models(test_query):\n",
    "    \"\"\"Compare responses from base and instruct models\"\"\"\n",
    "    print(f\"\\nTesting query: '{test_query}'\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Test with base model if available\n",
    "    if base_model and base_tokenizer:\n",
    "        base_response = generate_response_old(\n",
    "            test_query, \n",
    "            base_model, \n",
    "            base_tokenizer,\n",
    "            max_new_tokens=100,\n",
    "            temperature=0.1\n",
    "        )\n",
    "        print(f\"\\nBase Model Response:\\n{base_response}\")\n",
    "    else:\n",
    "        print(\"\\nBase model not available.\")\n",
    "    \n",
    "    # Test with instruct model if available\n",
    "    if instruct_model and instruct_tokenizer:\n",
    "        instruct_response = generate_response_old(\n",
    "            test_query, \n",
    "            instruct_model, \n",
    "            instruct_tokenizer,\n",
    "            max_new_tokens=100,\n",
    "            temperature=0.1\n",
    "        )\n",
    "        print(f\"\\nInstruct Model Response:\\n{instruct_response}\")\n",
    "    else:\n",
    "        print(\"\\nInstruct model not available.\")\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "\n",
    "# Test queries for model comparison\n",
    "test_queries = [\n",
    "    \"What is a buffer overflow attack?\",\n",
    "    \"Explain SQL injection\",\n",
    "    \"How does phishing work?\"\n",
    "]\n",
    "\n",
    "for query in test_queries:\n",
    "    compare_models(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing and Evaluation\n",
    "\n",
    "Let's test our optimized chatbot with various cybersecurity queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test queries for evaluation\n",
    "test_queries = [\n",
    "    \"What is Log4Shell (CVE-2021-44228) and how does it work?\",\n",
    "    \"Explain the difference between symmetric and asymmetric encryption\",\n",
    "    \"What are the OWASP Top 10 web application vulnerabilities?\",\n",
    "    \"How does a man-in-the-middle attack work?\",\n",
    "    \"What is zero-trust security architecture?\",\n",
    "    \"Explain the concept of defense-in-depth in cybersecurity\",\n",
    "    \"What is the difference between vulnerability and exploit?\",\n",
    "    \"How can I protect my organization from ransomware attacks?\"\n",
    "]\n",
    "\n",
    "print(\"Testing Security AI Assistant with various queries:\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for i, query in enumerate(test_queries, 1):\n",
    "    print(f\"\\n{i}. Query: {query}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    # Generate response\n",
    "    result = security_bot.generate_response(query, max_new_tokens=200)\n",
    "    response = result['response']\n",
    "    confidence = result['confidence']\n",
    "    \n",
    "    print(f\"Response: {response}\")\n",
    "    print(f\"Confidence: {confidence:.2f}\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "print(\"\\nTesting complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Metrics\n",
    "\n",
    "Let's analyze the performance of our cybersecurity assistant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_performance(test_queries):\n",
    "    \"\"\"Analyze the performance of the security chatbot\"\"\"\n",
    "    total_queries = len(test_queries)\n",
    "    successful_responses = 0\n",
    "    high_confidence_responses = 0\n",
    "    avg_confidence = 0\n",
    "    total_response_length = 0\n",
    "    \n",
    "    confidence_scores = []\n",
    "    response_lengths = []\n",
    "    \n",
    "    print(\"Analyzing performance metrics...\\n\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for i, query in enumerate(test_queries, 1):\n",
    "        print(f\"Processing query {i}/{total_queries}...\")\n",
    "        \n",
    "        # Generate response\n",
    "        result = security_bot.generate_response(query, max_new_tokens=200)\n",
    "        \n",
    "        if result['response'] and \"Error\" not in result['response']:\n",
    "            successful_responses += 1\n",
    "            \n",
    "            confidence = result['confidence']\n",
    "            confidence_scores.append(confidence)\n",
    "            \n",
    "            if confidence > 0.7:\n",
    "                high_confidence_responses += 1\n",
    "            \n",
    "            response_length = len(result['response'])\n",
    "            response_lengths.append(response_length)\n",
    "            total_response_length += response_length\n",
    "    \n",
    "    # Calculate metrics\n",
    "    success_rate = (successful_responses / total_queries) * 100 if total_queries > 0 else 0\n",
    "    high_confidence_rate = (high_confidence_responses / successful_responses) * 100 if successful_responses > 0 else 0\n",
    "    avg_confidence = sum(confidence_scores) / len(confidence_scores) if confidence_scores else 0\n",
    "    avg_response_length = total_response_length / successful_responses if successful_responses > 0 else 0\n",
    "    \n",
    "    # Display metrics\n",
    "    print(\"\\nPerformance Metrics:\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Total Queries: {total_queries}\")\n",
    "    print(f\"Successful Responses: {successful_responses}\")\n",
    "    print(f\"Success Rate: {success_rate:.1f}%\")\n",
    "    print(f\"High Confidence Responses: {high_confidence_responses}\")\n",
    "    print(f\"High Confidence Rate: {high_confidence_rate:.1f}%\")\n",
    "    print(f\"Average Confidence: {avg_confidence:.2f}\")\n",
    "    print(f\"Average Response Length: {avg_response_length:.0f} characters\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Return metrics\n",
    "    return {\n",
    "        'total_queries': total_queries,\n",
    "        'successful_responses': successful_responses,\n",
    "        'success_rate': success_rate,\n",
    "        'high_confidence_responses': high_confidence_responses,\n",
    "        'high_confidence_rate': high_confidence_rate,\n",
    "        'avg_confidence': avg_confidence,\n",
    "        'avg_response_length': avg_response_length\n",
    "    }\n",
    "\n",
    "# Run performance analysis\n",
    "performance_metrics = analyze_performance(test_queries)\n",
    "\n",
    "# Display model info\n",
    "model_info = security_bot.get_model_info()\n",
    "print(\"\\nModel Information:\")\n",
    "print(\"=\" * 60)\n",
    "for key, value in model_info.items():\n",
    "    print(f\"{key.replace('_', ' ').title()}: {value}\")\n",
    "print(\"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
